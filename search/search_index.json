{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 Welcome to the EMR Best Practices Guides. The primary goal of this project is to offer a set of best practices for day 2 operations for Amazon EMR. We elected to publish this guidance to GitHub so we could interate quickly, provide timely and effective recommendations for variety of concerns, and easily incorporate suggestions from the broader community. We currently have published guides for the following topics: Cost Optimizations and Reliability In the future we will be publishing best practices guidance for performance, cost optimization, and operational excellence. Contributing \u00b6 We encourage you to contribute to these guides. If you have implemented a practice that has proven to be effective, please share it with us by opening an issue or a pull request. Similarly, if you discover an error or flaw in the guidance we've already published, please submit a PR to correct it.","title":"Introduction"},{"location":"#introduction","text":"Welcome to the EMR Best Practices Guides. The primary goal of this project is to offer a set of best practices for day 2 operations for Amazon EMR. We elected to publish this guidance to GitHub so we could interate quickly, provide timely and effective recommendations for variety of concerns, and easily incorporate suggestions from the broader community. We currently have published guides for the following topics: Cost Optimizations and Reliability In the future we will be publishing best practices guidance for performance, cost optimization, and operational excellence.","title":"Introduction"},{"location":"#contributing","text":"We encourage you to contribute to these guides. If you have implemented a practice that has proven to be effective, please share it with us by opening an issue or a pull request. Similarly, if you discover an error or flaw in the guidance we've already published, please submit a PR to correct it.","title":"Contributing"},{"location":"applications/spark/","text":"Coming Soon","title":"Spark"},{"location":"cost_optimization/best_practices/","text":"1 - Cost Optimizations \u00b6 Best Practices (BP) for running cost optimized workloads on EMR. BP 1.1 Use Amazon S3 as your persistent data store \u00b6 As of Oct 1, 2021, Amazon S3 is 2.3 cents a GB/month for the first 50TB. This is $275 per TB/year which is a much lower cost than 3x replicated data in HDFS. With HDFS, you\u2019ll need to provision EBS volumes. EBS is 10 cents a GB/month, which is ~ 4x the cost of Amazon S3 or 12x if you include the need for 3x HDFS replication. Using Amazon S3 as your persistent data store allows you to grow your storage infinitely, independent of your compute. With on premise Hadoop systems, you would have to add nodes just to house your data which may not be helping your compute and only increase cost. In addition, Amazon S3 also has different storage tiers for less frequently accessed data providing opportunity for additional cost savings. EMR makes using Amazon S3 simple with EMR File System (EMRFS). EMRFS is an implementation of HDFS that all EMR clusters use for accessing data in Amazon S3. Note: HDFS is still available on the cluster if you need it and can be more performant compared to Amazon S3. HDFS on EMR uses EBS local block store which is faster than Amazon S3 object store. Some amounts of HDFS/EBS may be still be required. You may benefit from using HDFS for intermediate storage or need it to store application jars. However, HDFS is not recommended for persistent storage. Once a cluster is terminated, all HDFS data is lost. BP 1.2 Compress, compact and convert your Amazon S3 Objects \u00b6 Compress - By compressing your data, you reduce the amount of storage needed for the data, and minimize the network traffic between S3 and the EMR nodes. When you compress your data, make sure to use a compression algorithm that allows files to be split or have each file be the optimal size for parallelization on your cluster. File formats such as Apache Parquet or Apache ORC provide compression by default. The following image shows the size difference between two file formats, Parquet (has compression enabled) and JSON (text format, no compression enabled). The Parquet dataset is almost five times smaller than the JSON dataset despite having the same data. Compact - Avoid small files. Generally, anything less than 128 MB. By having fewer files that are larger, you can reduce the amount of Amazon S3 LIST requests and also improve the job performance. To show the performance impact of having too many files, the following image shows a query executed over a dataset containing 50 files and a query over a dataset of the same size, but with 25,000 files. The query that executed on 1 file is 3.6x faster despite the tables and records being the same. Convert - Columnar file formats like Parquet and ORC can improve read performance. Columnar formats are ideal if most of your queries only select a subset of columns. For use cases where you primarily select all columns, but only select a subset of rows, choose a row optimized file format such as Apache Avro. The following image shows a performance comparison of a select count( * ) query between Parquet and JSON (text) file formats. The query that executed over parquet ran 74x faster despite being larger in size. BP 1.3 Partition and Bucket your data in Amazon S3 \u00b6 Partition your data in Amazon S3 to reduce the amount of data that needs to be processed. When your applications or users access the data with the partition key, it only retrieves the objects that are required. This reduces the amount of data scanned and the amount of processing required for your job to run. This results in lower cost. For example, the following image shows two queries executed on two datasets of the same size. One dataset is partitioned, and the other dataset is not. The query over the partitioned data (s3logsjsonpartitioned) took 20 seconds to complete and it scanned 349 MB of data. The query over the non-partitioned data (s3logsjsonnopartition) took 2 minutes and 48 seconds to complete and it scanned 5.13 GB of data. Bucketing is another strategy that breaks down your data into ranges in order to minimize the amount of data scanned. This makes your query more efficient and reduces your job run time. The range for a bucket is determined by the hash value of one or more columns in the dataset. These columns are referred to as bucketing or clustered by columns. A bucketed table can be created as in the below example: CREATE TABLE IF NOT EXISTS database1 . table1 ( col1 INT , col2 STRING , col3 TIMESTAMP ) CLUSTERED BY ( col1 ) INTO 5 BUCKETS STORED AS PARQUET LOCATION \u2018 s3 :/// buckets_test / hive-clustered / \u2019 ; In this example, the bucketing column (col1) is specified by the CLUSTERED BY (col1) clause, and the number of buckets (5) is specified by the INTO 5 BUCKETS clause. Bucketing is similar to partitioning \u2013 in both cases, data is segregated and stored \u2013 but there are a few key differences. Partitioning is based on a column that is repeated in the dataset and involves grouping data by a particular value of the partition column. While bucketing organizes data by a range of values, mainly involving primary key or non-repeated values in a dataset. Bucketing should be considered when your partitions are not comparatively equal in size or you have data skew with your keys. Certain operations like map-side joins are more efficient in bucket tables vs non bucketed ones. BP 1.4 Use the right hardware family for the job type \u00b6 Most Amazon EMR clusters can run on general-purpose EC2 instance types/families such as m5.xlarge and m6g.xlarge. Compute-intensive clusters may benefit from running on high performance computing (HPC) instances, such as the compute-optimized instance family (C5). High memory-caching spark applications may benefit from running on high memory instances, such as the memory-optimized instance family (R5). Each of the different instance families have a different core:memory ratio so depending on your application characteristic, you should choose accordingly. The master node does not have large computational requirements. For most clusters of 50 or fewer nodes, you can use a general-purpose instance type such as m5. However, the master node is responsible for running key services such as Resource manager, Namenode, Hiveserver2 as such, it\u2019s recommended to use a larger instance such as 8xlarge+. With single node EMR cluster, the master node is a single point of failure. BP 1.5 Use instances with instance store for jobs that require high disk IOPS \u00b6 Use dense SSD storage instances for data-intensive workloads such as I3en or d3en. These instances provide Non-Volatile Memory Express (NVMe) SSD-backed instance storage optimized for low latency, very high random I/O performance, high sequential read throughput and provide high IOPS at a low cost. EMR workloads that spend heavily use HDFS or spend a lot of time writing spark shuffle data can benefit from these instances and see improved performance which reduces overall cost. BP 1.6 Use Graviton2 instances \u00b6 Amazon EMR supports Amazon EC2 graviton instances with EMR Versions 6.1.0, 5.31.0 and later. These instances are powered by AWS Graviton2 processors that are custom designed by AWS utilizing 64-bit ArmNeoverse cores to deliver the best price performance for cloud workloads running in Amazon EC2. On Graviton2 instances, Amazon EMR runtime for Apache Spark provides an additional cost savings of up to 30%, and improved performance of up to 15% relative to equivalent previous generation instances. For example, when you compare m5.4xlarge vs m6g.4xlarge. The total cost (EC2+EMR) / hour is Instance Type EC2 + EMR Cost m5.4xlarge: $0.960 m6g.4xlarge: $0.770 This is a 19.8% reduction in cost for the same amount of compute - 16vCPU and 64Gib Memory For more information, see: https://aws.amazon.com/blogs/big-data/amazon-emr-now-provides-up-to-30-lower-cost-and-up-to-15-improved-performance-for-spark-workloads-on-graviton2-based-instances BP 1.7 Select the appropriate pricing model for your use case and node type \u00b6 The following table is general guideline for purchasing options depending on your application scenario. Application scenario Master node purchasing option Core nodes purchasing option Task nodes purchasing option Long-running clusters and data warehouses On-Demand On-Demand or instance-fleet mix Spot or instance-fleet mix Cost-driven workloads Spot Spot Spot Data-critical workloads On-Demand On-Demand Spot or instance-fleet mix Application testing Spot Spot Spot For clusters where you need a minimum compute at all times - e.g spark streaming, ad hoc clusters. Using reserved instances or saving plans is recommended. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-purchasing-options.html BP 1.8 Use spot instances \u00b6 Spot instances are unused EC2 Capacity that is offered at up to a 90% discount (vs On-Demand pricing) and should be used when applicable. While EC2 can reclaim Spot capacity with a two-minute warning, less than 5% of workloads are interrupted. Due to the fault-tolerant nature of big data workloads on EMR, they can continue processing, even when interrupted. Running EMR on Spot Instances drastically reduces the cost of big data, allows for significantly higher compute capacity, and reduces the time to process big data sets. Below are the considerations and best practices when using Spot Instances on your EMR cluster. Use Spot for workloads where they can be interrupted and resumed (interruption rates are extremely low), or workloads that can exceed an SLA Use Spot for testing and development workloads or when testing testing new applications. Use Spot in combination with On demand as burst capacity to bring down total cluster cost while reducing job run time. See BP 1.11 Avoid spot if your workload requires predictable completion time or has service level agreement (SLA) requirements Use instance fleet with allocation strategy while using Spot so that you can diversify across many different instances. Spot capacity pool is unpredictable so diversifying with as many instances that meets your requirements can help increase the likelihood of securing spot instances which in turn, reduces cost. For more information, see: AWS Big Data Blog: Best practices for running Apache Spark applications using Amazon EC2 Spot Instances with Amazon EMR https://aws.amazon.com/blogs/big-data/best-practices-for-running-apache-spark-applications-using-amazon-ec2-spot-instances-with-amazon-emr/ Amazon EMR Cluster configuration guidelines and best practices https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html#emr-plan-spot-instances BP 1.9 Mix on-Demand and spot instances \u00b6 Consider using a combination of Spot and On-Demand instances to lower cost and runtime. Two examples where where this may be applicable are when: 1) Cost is more important than the time to completion, but you cannot tolerate an entire cluster being terminated. In this case, you can use Spot instances for the task nodes, and use On-Demand/Reserved instances for the master and core nodes. Even if all spot nodes are reclaimed, your cluster will still be accessible and tasks will be re-run on the remaining core nodes. 2) You need to meet SLAs but are also considered about cost In this case, you would provision enough on demand capacity to meet your SLAs and then use additional spot to bring down your average cost. If spot is not available, you\u2019ll still have on demand nodes to meet your SLA. When spot is available, your cluster will have additional compute which reduce run time and lowers the total cost of your job. For example: 10 node cluster run ning for 14 hours Cos t = 1.0 * 10 * 14 = $ 140 Add 10 more nodes on Spot at 0.5 $ / node 20 node cluster run ning for 7 hours Cos t = 1.0 * 10 * 7 = $ 70 = 0.5 * 10 * 7 = $ 35 To tal $ 105 50 % less run - time ( 14 \u2192 7 ) 25 % less cos t ( 140 \u2192 105 ) One consideration when mixing on demand and spot is if spot nodes are reclaimed, tasks or shuffle data that were on those spot nodes may have to be re executed on the remaining nodes. This reprocessing would increase the total run time of the job compared to running on only on demand. BP 1.10 Use EMR managed scaling \u00b6 With Amazon EMR versions 5.30.0 and later (except for Amazon EMR 6.0.0), you can enable EMR managed scaling. Managed scaling lets you automatically increase or decrease the number of instances or units in your cluster based on workload. EMR continuously evaluates cluster metrics to make scaling decisions that optimize your clusters for cost and speed improving overall cluster utilization. Managed scaling is available for clusters composed of either instance groups or instance fleets This helps you reduce costs by running your EMR clusters with just the correct of amount of resources that your application needs. This feature is also useful for use cases where you have spikes in cluster utilization (i.e. a user submitting a job) and you want the cluster to automatically scale based on the requirements for that application. Here\u2019s an example of cluster without auto scaling. Since the size of the cluster is static, there are resources you are paying for but your job does not actually need. Here\u2019s an example of cluster with auto scaling. The cluster capacity (blue dotted line) adjusts to the job demand reducing unused resources and cost. BP 1.11 Right size application containers \u00b6 By default, EMR will try to set YARN and Spark memory settings to best utilize the instances compute resources. This is important to maximize your cluster resources. Whether you are migrating jobs to EMR or writing a new application, It is recommended that you start with default EMR configuration. If you need to modify the default configuration for your specific use case, It\u2019s important to use all the available resources of the cluster - both CPU and Memory. For example, if you had a cluster that is using m5.4xlarge instances for its data nodes, you\u2019d have 16 vCPU and 64GB of memory. EMR will automatically set yarn.nodemanager.resource.cpu-vcores and yarn.nodemanager.resource.memory-mb in yarn-site.xml to allocate how much of the instances resources can be used for YARN applications. In the m5.4xlarge case, this is 16vCPU and 57344 mb. When using custom configuration for your spark containers, you want to ensure that the memory and cores you allocate to your executor is a multiple of the total resources allocated to yarn. For example, if you set spark.executor.memory 20,000M spark.yarn.executor.memoryOverhead 10% (2,000M) spark.executor.cores 4 Spark will only be able to allocate 2 executors on each node resulting in 57,344-44,000 (22,000 * 2) = 13,344 of unallocated resources and 76.7% memory utilization However, if spark.executor.memory was right sized to the available total yarn.nodemanager.resource.memory-mb you would get higher instance utilization. For example, spark.executor.memory 12,000M spark.yarn.executor.memoryOverhead 10% (1,200M) spark.executor.cores 4 Spark will be able to allocate 4 executors on each node resulting in only 57,344-52,800(13,200 * 4) = 4,544 of unallocated resources and 92.0% memory utilization For more information on Spark and YARN right sizing see: https://aws.amazon.com/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/ https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hadoop-task-config.html BP 1.12 Monitor cluster utilization \u00b6 Monitoring cluster utilization is important for right sizing your cluster which can help reduces costs. To monitor cluster utilization, you can use EMR cloudwatch metrics, Ganglia (can be installed with EMR) or configure a 3rd party tool like Grafana and Prometheus. Regardless of which tool you use, you\u2019ll want to monitor cluster metrics such as available vCPU, Memory and disk utilization to determine if you\u2019re right sized for your workload. If some containers are constantly available, shrinking your cluster saves cost without decreasing performance because containers are sitting idle. For example, If looking at Ganglia shows that either CPU or memory is 100% but the other resources are not being used significantly, then consider moving to another instance type that may provide better performance at a lower cost or reducing the total cluster size. For example, if CPU is 100%, and memory usage is less than 50% on R4 or M5 series instance types, then moving to C4 series instance type may be able to address the bottleneck on CPU. If both CPU and memory usage is at 50%, reducing cluster capacity in half could give you the same performance at half the cost These recommendations are more applicable towards job scoped pipelines or transient clusters where the workload pattern is known or constant. If the cluster is long running or the workload pattern is not predictable, using managed scaling should be considered since it will attempt to rightsize the cluster automatically. For more information on which cloudwatch metrics are available, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html For more information on the Grafana and Prometheus solution, see: https://aws.amazon.com/blogs/big-data/monitor-and-optimize-analytic-workloads-on-amazon-emr-with-prometheus-and-grafana/ BP 1.13 Monitor and decommission idle EMR cluster \u00b6 Decommission Amazon EMR clusters that are no longer required to lower cost. This can be achieved in two ways. You can use EMR\u2019s \u201cautomatic termination policy\u201d starting 5.30.0 and 6.1.0 or, by monitoring the \u201cisIdle\u201d metric in cloudwatch and terminating yourself. With EMR\u2019s automatic termination policy feature, EMR continuously samples key metrics associated with the workloads running on the clusters, and auto-terminates when the cluster is idle. For more information on when a cluster is considered idle and considerations, see https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-auto-termination-policy.html With EMR\u2019s \u201cisIdle\u201d cloudwatch metric, EMR will emit 1 if no tasks are running and no jobs are running, and emit 0 otherwise. This value is checked at five-minute intervals and a value of 1 indicates only that the cluster was idle when checked, not that it was idle for the entire five minutes. You can set an alarm to fire when the cluster has been idle for a given period of time, such as thirty minutes. Non-YARN based applications such as Presto, Trino, or HBase are not considered with the \u201cIsIdle\u201d Metrics For a sample solution of this approach, see https://aws.amazon.com/blogs/big-data/optimize-amazon-emr-costs-with-idle-checks-and-automatic-resource-termination-using-advanced-amazon-cloudwatch-metrics-and-aws-lambda/ BP 1.14 Use the latest Amazon EMR version \u00b6 Use the latest EMR version and upgrade whenever possible. New EMR versions have performance improvements, cost savings, bug fixes stability improvements and new features. For more information, see EMR Release Guide https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-components.html BP 1.15 Using transient and long-running clusters \u00b6 Amazon EMR supports both transient clusters and long running clusters and both should be considered depending on your use case and job type. In general, transient clusters are good for job scoped pipelines. Clusters can be right-sized to meet the exact needs of your job. Using transient clusters reduces the blast radius across other jobs and makes it easier to upgrade clusters and restart jobs. Since transient clusters are shutdown after the job is run, you don\u2019t need to worry about idle resources and managing many aspects of cluster life cycle, including replacing failed nodes, upgrades, patching, etc. In general, long running clusters are good for short-running jobs, ad hoc queries and streaming applications. Long running clusters can also be considered to save costs and operations for multi tenanted data science and engineering jobs. From a cost optimization standpoint, If using transient clusters, ensure your instances and containers are right sized so that you are not over provisioned. Use BP 1.12 to determine cluster utilization and if you\u2019re able to lower your requested compute while still meeting your SLA. If using long running clusters, ensure you\u2019re using EMR Managed Scaling to scale up and down resources based off your jobs needs. It is also important to treat the cluster as a transient resources and have the automation in place to decommission and restart clusters.","title":"Best Practices"},{"location":"cost_optimization/best_practices/#1-cost-optimizations","text":"Best Practices (BP) for running cost optimized workloads on EMR.","title":"1 - Cost Optimizations"},{"location":"cost_optimization/best_practices/#bp-11-use-amazon-s3-as-your-persistent-data-store","text":"As of Oct 1, 2021, Amazon S3 is 2.3 cents a GB/month for the first 50TB. This is $275 per TB/year which is a much lower cost than 3x replicated data in HDFS. With HDFS, you\u2019ll need to provision EBS volumes. EBS is 10 cents a GB/month, which is ~ 4x the cost of Amazon S3 or 12x if you include the need for 3x HDFS replication. Using Amazon S3 as your persistent data store allows you to grow your storage infinitely, independent of your compute. With on premise Hadoop systems, you would have to add nodes just to house your data which may not be helping your compute and only increase cost. In addition, Amazon S3 also has different storage tiers for less frequently accessed data providing opportunity for additional cost savings. EMR makes using Amazon S3 simple with EMR File System (EMRFS). EMRFS is an implementation of HDFS that all EMR clusters use for accessing data in Amazon S3. Note: HDFS is still available on the cluster if you need it and can be more performant compared to Amazon S3. HDFS on EMR uses EBS local block store which is faster than Amazon S3 object store. Some amounts of HDFS/EBS may be still be required. You may benefit from using HDFS for intermediate storage or need it to store application jars. However, HDFS is not recommended for persistent storage. Once a cluster is terminated, all HDFS data is lost.","title":"BP 1.1 Use Amazon S3 as your persistent data store"},{"location":"cost_optimization/best_practices/#bp-12-compress-compact-and-convert-your-amazon-s3-objects","text":"Compress - By compressing your data, you reduce the amount of storage needed for the data, and minimize the network traffic between S3 and the EMR nodes. When you compress your data, make sure to use a compression algorithm that allows files to be split or have each file be the optimal size for parallelization on your cluster. File formats such as Apache Parquet or Apache ORC provide compression by default. The following image shows the size difference between two file formats, Parquet (has compression enabled) and JSON (text format, no compression enabled). The Parquet dataset is almost five times smaller than the JSON dataset despite having the same data. Compact - Avoid small files. Generally, anything less than 128 MB. By having fewer files that are larger, you can reduce the amount of Amazon S3 LIST requests and also improve the job performance. To show the performance impact of having too many files, the following image shows a query executed over a dataset containing 50 files and a query over a dataset of the same size, but with 25,000 files. The query that executed on 1 file is 3.6x faster despite the tables and records being the same. Convert - Columnar file formats like Parquet and ORC can improve read performance. Columnar formats are ideal if most of your queries only select a subset of columns. For use cases where you primarily select all columns, but only select a subset of rows, choose a row optimized file format such as Apache Avro. The following image shows a performance comparison of a select count( * ) query between Parquet and JSON (text) file formats. The query that executed over parquet ran 74x faster despite being larger in size.","title":"BP 1.2 Compress, compact and convert your Amazon S3 Objects"},{"location":"cost_optimization/best_practices/#bp-13-partition-and-bucket-your-data-in-amazon-s3","text":"Partition your data in Amazon S3 to reduce the amount of data that needs to be processed. When your applications or users access the data with the partition key, it only retrieves the objects that are required. This reduces the amount of data scanned and the amount of processing required for your job to run. This results in lower cost. For example, the following image shows two queries executed on two datasets of the same size. One dataset is partitioned, and the other dataset is not. The query over the partitioned data (s3logsjsonpartitioned) took 20 seconds to complete and it scanned 349 MB of data. The query over the non-partitioned data (s3logsjsonnopartition) took 2 minutes and 48 seconds to complete and it scanned 5.13 GB of data. Bucketing is another strategy that breaks down your data into ranges in order to minimize the amount of data scanned. This makes your query more efficient and reduces your job run time. The range for a bucket is determined by the hash value of one or more columns in the dataset. These columns are referred to as bucketing or clustered by columns. A bucketed table can be created as in the below example: CREATE TABLE IF NOT EXISTS database1 . table1 ( col1 INT , col2 STRING , col3 TIMESTAMP ) CLUSTERED BY ( col1 ) INTO 5 BUCKETS STORED AS PARQUET LOCATION \u2018 s3 :/// buckets_test / hive-clustered / \u2019 ; In this example, the bucketing column (col1) is specified by the CLUSTERED BY (col1) clause, and the number of buckets (5) is specified by the INTO 5 BUCKETS clause. Bucketing is similar to partitioning \u2013 in both cases, data is segregated and stored \u2013 but there are a few key differences. Partitioning is based on a column that is repeated in the dataset and involves grouping data by a particular value of the partition column. While bucketing organizes data by a range of values, mainly involving primary key or non-repeated values in a dataset. Bucketing should be considered when your partitions are not comparatively equal in size or you have data skew with your keys. Certain operations like map-side joins are more efficient in bucket tables vs non bucketed ones.","title":"BP 1.3 Partition and Bucket your data in Amazon S3"},{"location":"cost_optimization/best_practices/#bp-14-use-the-right-hardware-family-for-the-job-type","text":"Most Amazon EMR clusters can run on general-purpose EC2 instance types/families such as m5.xlarge and m6g.xlarge. Compute-intensive clusters may benefit from running on high performance computing (HPC) instances, such as the compute-optimized instance family (C5). High memory-caching spark applications may benefit from running on high memory instances, such as the memory-optimized instance family (R5). Each of the different instance families have a different core:memory ratio so depending on your application characteristic, you should choose accordingly. The master node does not have large computational requirements. For most clusters of 50 or fewer nodes, you can use a general-purpose instance type such as m5. However, the master node is responsible for running key services such as Resource manager, Namenode, Hiveserver2 as such, it\u2019s recommended to use a larger instance such as 8xlarge+. With single node EMR cluster, the master node is a single point of failure.","title":"BP 1.4 Use the right hardware family for the job type"},{"location":"cost_optimization/best_practices/#bp-15-use-instances-with-instance-store-for-jobs-that-require-high-disk-iops","text":"Use dense SSD storage instances for data-intensive workloads such as I3en or d3en. These instances provide Non-Volatile Memory Express (NVMe) SSD-backed instance storage optimized for low latency, very high random I/O performance, high sequential read throughput and provide high IOPS at a low cost. EMR workloads that spend heavily use HDFS or spend a lot of time writing spark shuffle data can benefit from these instances and see improved performance which reduces overall cost.","title":"BP 1.5 Use instances with instance store for jobs that require high disk IOPS"},{"location":"cost_optimization/best_practices/#bp-16-use-graviton2-instances","text":"Amazon EMR supports Amazon EC2 graviton instances with EMR Versions 6.1.0, 5.31.0 and later. These instances are powered by AWS Graviton2 processors that are custom designed by AWS utilizing 64-bit ArmNeoverse cores to deliver the best price performance for cloud workloads running in Amazon EC2. On Graviton2 instances, Amazon EMR runtime for Apache Spark provides an additional cost savings of up to 30%, and improved performance of up to 15% relative to equivalent previous generation instances. For example, when you compare m5.4xlarge vs m6g.4xlarge. The total cost (EC2+EMR) / hour is Instance Type EC2 + EMR Cost m5.4xlarge: $0.960 m6g.4xlarge: $0.770 This is a 19.8% reduction in cost for the same amount of compute - 16vCPU and 64Gib Memory For more information, see: https://aws.amazon.com/blogs/big-data/amazon-emr-now-provides-up-to-30-lower-cost-and-up-to-15-improved-performance-for-spark-workloads-on-graviton2-based-instances","title":"BP 1.6 Use Graviton2 instances"},{"location":"cost_optimization/best_practices/#bp-17-select-the-appropriate-pricing-model-for-your-use-case-and-node-type","text":"The following table is general guideline for purchasing options depending on your application scenario. Application scenario Master node purchasing option Core nodes purchasing option Task nodes purchasing option Long-running clusters and data warehouses On-Demand On-Demand or instance-fleet mix Spot or instance-fleet mix Cost-driven workloads Spot Spot Spot Data-critical workloads On-Demand On-Demand Spot or instance-fleet mix Application testing Spot Spot Spot For clusters where you need a minimum compute at all times - e.g spark streaming, ad hoc clusters. Using reserved instances or saving plans is recommended. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-purchasing-options.html","title":"BP 1.7 Select the appropriate pricing model for your use case and node type"},{"location":"cost_optimization/best_practices/#bp-18-use-spot-instances","text":"Spot instances are unused EC2 Capacity that is offered at up to a 90% discount (vs On-Demand pricing) and should be used when applicable. While EC2 can reclaim Spot capacity with a two-minute warning, less than 5% of workloads are interrupted. Due to the fault-tolerant nature of big data workloads on EMR, they can continue processing, even when interrupted. Running EMR on Spot Instances drastically reduces the cost of big data, allows for significantly higher compute capacity, and reduces the time to process big data sets. Below are the considerations and best practices when using Spot Instances on your EMR cluster. Use Spot for workloads where they can be interrupted and resumed (interruption rates are extremely low), or workloads that can exceed an SLA Use Spot for testing and development workloads or when testing testing new applications. Use Spot in combination with On demand as burst capacity to bring down total cluster cost while reducing job run time. See BP 1.11 Avoid spot if your workload requires predictable completion time or has service level agreement (SLA) requirements Use instance fleet with allocation strategy while using Spot so that you can diversify across many different instances. Spot capacity pool is unpredictable so diversifying with as many instances that meets your requirements can help increase the likelihood of securing spot instances which in turn, reduces cost. For more information, see: AWS Big Data Blog: Best practices for running Apache Spark applications using Amazon EC2 Spot Instances with Amazon EMR https://aws.amazon.com/blogs/big-data/best-practices-for-running-apache-spark-applications-using-amazon-ec2-spot-instances-with-amazon-emr/ Amazon EMR Cluster configuration guidelines and best practices https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html#emr-plan-spot-instances","title":"BP 1.8 Use spot instances"},{"location":"cost_optimization/best_practices/#bp-19-mix-on-demand-and-spot-instances","text":"Consider using a combination of Spot and On-Demand instances to lower cost and runtime. Two examples where where this may be applicable are when: 1) Cost is more important than the time to completion, but you cannot tolerate an entire cluster being terminated. In this case, you can use Spot instances for the task nodes, and use On-Demand/Reserved instances for the master and core nodes. Even if all spot nodes are reclaimed, your cluster will still be accessible and tasks will be re-run on the remaining core nodes. 2) You need to meet SLAs but are also considered about cost In this case, you would provision enough on demand capacity to meet your SLAs and then use additional spot to bring down your average cost. If spot is not available, you\u2019ll still have on demand nodes to meet your SLA. When spot is available, your cluster will have additional compute which reduce run time and lowers the total cost of your job. For example: 10 node cluster run ning for 14 hours Cos t = 1.0 * 10 * 14 = $ 140 Add 10 more nodes on Spot at 0.5 $ / node 20 node cluster run ning for 7 hours Cos t = 1.0 * 10 * 7 = $ 70 = 0.5 * 10 * 7 = $ 35 To tal $ 105 50 % less run - time ( 14 \u2192 7 ) 25 % less cos t ( 140 \u2192 105 ) One consideration when mixing on demand and spot is if spot nodes are reclaimed, tasks or shuffle data that were on those spot nodes may have to be re executed on the remaining nodes. This reprocessing would increase the total run time of the job compared to running on only on demand.","title":"BP 1.9 Mix on-Demand and spot instances"},{"location":"cost_optimization/best_practices/#bp-110-use-emr-managed-scaling","text":"With Amazon EMR versions 5.30.0 and later (except for Amazon EMR 6.0.0), you can enable EMR managed scaling. Managed scaling lets you automatically increase or decrease the number of instances or units in your cluster based on workload. EMR continuously evaluates cluster metrics to make scaling decisions that optimize your clusters for cost and speed improving overall cluster utilization. Managed scaling is available for clusters composed of either instance groups or instance fleets This helps you reduce costs by running your EMR clusters with just the correct of amount of resources that your application needs. This feature is also useful for use cases where you have spikes in cluster utilization (i.e. a user submitting a job) and you want the cluster to automatically scale based on the requirements for that application. Here\u2019s an example of cluster without auto scaling. Since the size of the cluster is static, there are resources you are paying for but your job does not actually need. Here\u2019s an example of cluster with auto scaling. The cluster capacity (blue dotted line) adjusts to the job demand reducing unused resources and cost.","title":"BP 1.10 Use EMR managed scaling"},{"location":"cost_optimization/best_practices/#bp-111-right-size-application-containers","text":"By default, EMR will try to set YARN and Spark memory settings to best utilize the instances compute resources. This is important to maximize your cluster resources. Whether you are migrating jobs to EMR or writing a new application, It is recommended that you start with default EMR configuration. If you need to modify the default configuration for your specific use case, It\u2019s important to use all the available resources of the cluster - both CPU and Memory. For example, if you had a cluster that is using m5.4xlarge instances for its data nodes, you\u2019d have 16 vCPU and 64GB of memory. EMR will automatically set yarn.nodemanager.resource.cpu-vcores and yarn.nodemanager.resource.memory-mb in yarn-site.xml to allocate how much of the instances resources can be used for YARN applications. In the m5.4xlarge case, this is 16vCPU and 57344 mb. When using custom configuration for your spark containers, you want to ensure that the memory and cores you allocate to your executor is a multiple of the total resources allocated to yarn. For example, if you set spark.executor.memory 20,000M spark.yarn.executor.memoryOverhead 10% (2,000M) spark.executor.cores 4 Spark will only be able to allocate 2 executors on each node resulting in 57,344-44,000 (22,000 * 2) = 13,344 of unallocated resources and 76.7% memory utilization However, if spark.executor.memory was right sized to the available total yarn.nodemanager.resource.memory-mb you would get higher instance utilization. For example, spark.executor.memory 12,000M spark.yarn.executor.memoryOverhead 10% (1,200M) spark.executor.cores 4 Spark will be able to allocate 4 executors on each node resulting in only 57,344-52,800(13,200 * 4) = 4,544 of unallocated resources and 92.0% memory utilization For more information on Spark and YARN right sizing see: https://aws.amazon.com/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/ https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hadoop-task-config.html","title":"BP 1.11 Right size application containers"},{"location":"cost_optimization/best_practices/#bp-112-monitor-cluster-utilization","text":"Monitoring cluster utilization is important for right sizing your cluster which can help reduces costs. To monitor cluster utilization, you can use EMR cloudwatch metrics, Ganglia (can be installed with EMR) or configure a 3rd party tool like Grafana and Prometheus. Regardless of which tool you use, you\u2019ll want to monitor cluster metrics such as available vCPU, Memory and disk utilization to determine if you\u2019re right sized for your workload. If some containers are constantly available, shrinking your cluster saves cost without decreasing performance because containers are sitting idle. For example, If looking at Ganglia shows that either CPU or memory is 100% but the other resources are not being used significantly, then consider moving to another instance type that may provide better performance at a lower cost or reducing the total cluster size. For example, if CPU is 100%, and memory usage is less than 50% on R4 or M5 series instance types, then moving to C4 series instance type may be able to address the bottleneck on CPU. If both CPU and memory usage is at 50%, reducing cluster capacity in half could give you the same performance at half the cost These recommendations are more applicable towards job scoped pipelines or transient clusters where the workload pattern is known or constant. If the cluster is long running or the workload pattern is not predictable, using managed scaling should be considered since it will attempt to rightsize the cluster automatically. For more information on which cloudwatch metrics are available, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html For more information on the Grafana and Prometheus solution, see: https://aws.amazon.com/blogs/big-data/monitor-and-optimize-analytic-workloads-on-amazon-emr-with-prometheus-and-grafana/","title":"BP 1.12 Monitor cluster utilization"},{"location":"cost_optimization/best_practices/#bp-113-monitor-and-decommission-idle-emr-cluster","text":"Decommission Amazon EMR clusters that are no longer required to lower cost. This can be achieved in two ways. You can use EMR\u2019s \u201cautomatic termination policy\u201d starting 5.30.0 and 6.1.0 or, by monitoring the \u201cisIdle\u201d metric in cloudwatch and terminating yourself. With EMR\u2019s automatic termination policy feature, EMR continuously samples key metrics associated with the workloads running on the clusters, and auto-terminates when the cluster is idle. For more information on when a cluster is considered idle and considerations, see https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-auto-termination-policy.html With EMR\u2019s \u201cisIdle\u201d cloudwatch metric, EMR will emit 1 if no tasks are running and no jobs are running, and emit 0 otherwise. This value is checked at five-minute intervals and a value of 1 indicates only that the cluster was idle when checked, not that it was idle for the entire five minutes. You can set an alarm to fire when the cluster has been idle for a given period of time, such as thirty minutes. Non-YARN based applications such as Presto, Trino, or HBase are not considered with the \u201cIsIdle\u201d Metrics For a sample solution of this approach, see https://aws.amazon.com/blogs/big-data/optimize-amazon-emr-costs-with-idle-checks-and-automatic-resource-termination-using-advanced-amazon-cloudwatch-metrics-and-aws-lambda/","title":"BP 1.13 Monitor and decommission idle EMR cluster"},{"location":"cost_optimization/best_practices/#bp-114-use-the-latest-amazon-emr-version","text":"Use the latest EMR version and upgrade whenever possible. New EMR versions have performance improvements, cost savings, bug fixes stability improvements and new features. For more information, see EMR Release Guide https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-components.html","title":"BP 1.14 Use the latest Amazon EMR version"},{"location":"cost_optimization/best_practices/#bp-115-using-transient-and-long-running-clusters","text":"Amazon EMR supports both transient clusters and long running clusters and both should be considered depending on your use case and job type. In general, transient clusters are good for job scoped pipelines. Clusters can be right-sized to meet the exact needs of your job. Using transient clusters reduces the blast radius across other jobs and makes it easier to upgrade clusters and restart jobs. Since transient clusters are shutdown after the job is run, you don\u2019t need to worry about idle resources and managing many aspects of cluster life cycle, including replacing failed nodes, upgrades, patching, etc. In general, long running clusters are good for short-running jobs, ad hoc queries and streaming applications. Long running clusters can also be considered to save costs and operations for multi tenanted data science and engineering jobs. From a cost optimization standpoint, If using transient clusters, ensure your instances and containers are right sized so that you are not over provisioned. Use BP 1.12 to determine cluster utilization and if you\u2019re able to lower your requested compute while still meeting your SLA. If using long running clusters, ensure you\u2019re using EMR Managed Scaling to scale up and down resources based off your jobs needs. It is also important to treat the cluster as a transient resources and have the automation in place to decommission and restart clusters.","title":"BP 1.15 Using transient and long-running clusters"},{"location":"cost_optimization/introduction/","text":"EMR Cost Optimization best practices focus on the continual process of refinement and improvement of a system over its entire lifecycle. From the initial design of your very first proof of concept to the ongoing operation of production workloads, adopting the practices in this document can enable you to build and operate cost-aware systems that achieve business outcomes and minimize costs, thus allowing your business to maximize its return on investment. A cost-optimized workload is one that Meets functional and non functional requirements Fully utilizes all cluster resources and Achieves an outcome at the lowest possible price point To better understand this, let\u2019s look at an example. Let\u2019s assume we have an ETL job that needs to be completed within 8 hours. In order to meet the requirements of completing the job within 8 hours, a certain amount of compute resources will be required. This is represented in the graph by the \u201cJob Demand\u201d. Sometimes this is static, where the amount of resources needed is consistent throughout the duration of the job. And sometimes it\u2019s more dynamic, where throughout the job, you have various peaks and valleys depending on the number of tasks that are running at each stage. In order for the job to finish within the 8 hours, it needs enough cluster capacity to meet the jobs compute demand - represented by the blue dotted line. If our cluster capacity is below our jobs compute demand Our job will be resource constrained and It\u2019ll cause the job to run longer than our 8 hour sla Now, just being able to meet your SLA is not enough to be cost optimized. This leads us to our 2nd step of a cost optimized workload - Fully utilizing all cluster resources Take these next two graphs as an example, in the first case, we have a cluster that has compute capacity well beyond the jobs needs, represented by space in between the jobs demand and cluster capacity In this 2nd graph, we have a better match between the clusters capacity and jobs compute demands The space in between in between two is unused resources. These are resources that are being charged for but the job does not actually need. Fully utilizing all resources means reducing this space as much as possible. Going back to our job with less predictable workload patterns, a static cluster size may not be the best way to maximize cluster resources, but instead, using something like EMR autoscaling that adjusts cluster capacity based off of your workload demand would be a better fit. In this graph, our cluster scales up and down depending on demand. Our cluster capacity becomes a function of the jobs demand of resources. The last part of being \u201cCost Optimized\u201d is achieving your jobs outcomes at the lowest price point possible. EMR has multiple pricing models that allow you to pay for your resources in the most cost-effective way that suits your needs. For example, On-Demand, Spot and Commitment discounts - Savings Plans/ Reserved Instances/Capacity All of these pricing options leverage the exact same infrastructure but depending on which option you choose, the cost of your job will vary significantly. The numbers are just examples, with spot you can get up to 90% off on demand prices and with saving plans or RI, up to 72%. In the next sections, we\u2019ll discuss best practices on choosing the right pricing model for your workload. For the purpose of this example, regardless of of which option you choose, the cluster compute capacity stays the same.","title":"Introduction"},{"location":"reliability/best_practices/","text":"2 - Reliability \u00b6 Best Practices (BP) for running reliable workloads on EMR. BP 2.1 Treat all clusters as transient resources \u00b6 Whether you use your EMR cluster as a long or short running cluster, treat them as transient resources. This means you have the automation in place to re-provision clusters on demand and have standard templates to ensure cluster startup consistency. Even if you are using a long running clusters, it\u2019s recommended to recreate the cluster during some periodical interval. Services integrated with clusters also need to be decoupled from the cluster. For example any persistent data, meta data, scripts, and job/work orchestrator's (e.g oozie and airflow) should be stored off cluster. Decoupling the cluster from these services minimizes blast radius in the event of a cluster failure and non impacted clusters can continue using these off-cluster services. There are several benefits to this approach. It makes upgrading, patching, rotating AMI\u2019s or making any other infrastructure changes easier. It allows you to quickly recover from failures and it removes the operational overhead of managing a long running cluster. You may also see an improvement in cost since clusters will only run for the duration of your job or use case. If you need to store state on cluster, ensure the state is backed up and synced. For more information on orchestrating transient EMR cluster, see: https://aws.amazon.com/blogs/aws/new-using-step-functions-to-orchestrate-amazon-emr-workloads/ https://aws.amazon.com/blogs/big-data/orchestrating-analytics-jobs-on-amazon-emr-notebooks-using-amazon-mwaa/ https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-longrunning-transient.html Specifically for EMR application logging, consider using EMR\u2019s Persistent Application User Interfaces (Spark, YARN RM, Tez UI, etc) which are hosted by EMR off cluster and available even after clusters are terminated. For more information on off cluster monitoring options, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/app-history-spark-UI.html https://aws.amazon.com/blogs/big-data/monitor-and-optimize-analytic-workloads-on-amazon-emr-with-prometheus-and-grafana/ For more information on external catalog, see: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-metastore-external-hive.html BP 2.2 Decouple storage and compute \u00b6 Store persistent data in Amazon S3 and use the EMR File System (EMRFS) for reading and writing data from Amazon EMR. EMRFS is an implementation of HDFS that all Amazon EMR clusters use for accessing data in Amazon S3. Applications such as Apache Hive and Apache Spark work with Amazon S3 by mapping the HDFS APIs to Amazon S3 APIs (like EMRFS available with Amazon EMR). You specify which file system to use by the prefix of the URI used to access the data. For example, s3://DOC-EXAMPLE-BUCKET1/path references an Amazon S3 bucket using EMRFS. By keeping persistent data in Amazon S3, you minimize the impact that infrastructure or service disruptions can have on your data. For example, in the event of an EC2 hardware failure during an application run, data in Amazon S3 will not be impacted. You can provision a new cluster and re run your application that points to the existing S3 bucket. From an application and user perspective, by decoupling storage and compute, you can point many EMR clusters at the same source of truth. If you have different departments that want to operate different jobs, they can act in isolation without affecting the core production of your environment. This also allows you to split interactive query workloads with ETL type workloads which gives you more flexibility in how you operate For example, In an Amazon EMR environment you can provision a new cluster with a new technology and operate it in parallel on your data with your core production environment. Once you make a decision on which technology to adopt, you can easily cut over from one to other. This allows future proofing and option value because you can keep pace the analytic tool set evolves, your infrastructure can evolve with it, without any expensive re platforming or re transformation of data. HDFS is still available on Amazon EMR clusters and is a good option for temporary or intermediate data. For example, workloads with iterative reads on the same data set or Disk I/O intensive workloads. For example, some hive jobs write a lot of data to HDFS, either staging data or through a multi step pipeline. It may be more cost efficient and performant to use HDFS for these stages compared to writing to Amazon S3. You lose the HDFS data once EMR clusters are terminated so this should only be used for intermediate or staging data. Another strategy is to ensure that when using HDFS, you checkpoint data at regular intervals so that if you lose cluster mid-work, you do not have to restart from scratch. Once data is written to HDFS, you can use something like s3distcp to move your data to Amazon S3. BP 2.3 Use the latest AMI and EMR version available \u00b6 In the Cost Optimization section, we talked about the benefits of using the latest EMR version. Equally important is using the latest AMI available. This ensures your up to date with the latest bug fixes, features and security updates. EMR allows has 2 AMI options available - default EMR AMI and Custom AMI. The default EMR AMI is based on the most up-to-date Amazon Linux AMI available at the time of the Amazon EMR release. Each Amazon EMR release version is \"locked\" to the Amazon Linux AMI version to maintain compatibility. This means that the same Amazon Linux AMI version is used for an Amazon EMR release version even when newer Amazon Linux AMIs become available. For this reason, we recommend that you use the latest Amazon EMR release version unless you need an earlier version for compatibility and are unable to migrate. When using a custom AMI, it is recommended to base your customization on the most recent EBS-backed Amazon Linux AMI (AL2 for 5.30.0 and later). Consider creating a new custom EMR AMI each time a new AL AMI is released. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-default-ami.html BP 2.4 Spread clusters across availability zones/subnets and time of provisioning \u00b6 Spread clusters across multiple Availability Zones (AZ) to provide resiliency against AZ failures. An added benefit is that it can help reduce insufficient capacity errors (ICE) since your EC2 requests are now across multiple EC2 pools. Instances of a single cluster can only be provisioned in a single AZ. EMR helps you achieve this with instance fleets. Instead of specifying a single Amazon EC2 availability zone for your Amazon EMR cluster and a specific Amazon EC2 instance type for an Amazon EMR instance group, you can provide a list of availability zones and instances, and Amazon EMR will automatically select an optimal combination based on cost and availability. For example, If Amazon EMR detects an AWS large-scale event in one or more of the Availability Zones, or cannot get enough capacity, Amazon EMR automatically attempts to route traffic away from the impacted Availability Zones and tries to launch clusters in alternate Availability Zones according to your selections. With Instance Groups, you must explicitly set the subnet at provisioning time. You can still spread clusters across your AZs by picking through round robin or at random. If your use case allows, spread cluster provisioning times across the hour or day to distribute your requests to EC2 instead of provisioning clusters at the same time. This decreases the likelihood of getting insufficient capacity errors. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html BP 2.5 Use on demand for core nodes and spot for task \u00b6 Core nodes run the Data Node daemon to coordinate data storage as part of the Hadoop Distributed File System (HDFS). If a core node is running on spot and the spot node is reclaimed, Hadoop has to re balance the data in HDFS to the remaining core nodes. If there are no core nodes remaining, you run the risk of losing HDFS data and the name node going into safe mode making the cluster unhealthy and usable. BP 2.6 Use instance fleet with allocation strategy \u00b6 The instance fleet configuration for Amazon EMR clusters lets you select a wide variety of provisioning options for Amazon EC2 instances, and helps you develop a flexible and elastic resourcing strategy for each node type in your cluster. You can have one instance fleet for each node group - master, core and task. Within the instance fleet, you specify a target capacity for on-demand and spot instances and with the allocation strategy option, you can select up to 30 instance types per fleet. In an instance fleet configuration, you specify a target capacity for On-Demand Instances and Spot Instances within each fleet. When the cluster launches, Amazon EMR provisions instances until the targets are fulfilled using any of the instances specified if your fleet. When Amazon EC2 reclaims a Spot Instance in a running cluster because of a price increase or instance failure, Amazon EMR tries to replace the instance with any of the instance types that you specify. This makes it easier to regain capacity during a spike in Spot pricing. It is recommended that you use the allocation strategy option for faster cluster provisioning, more accurate Spot Instance allocation, and fewer Spot Instance interruptions. With the allocation strategy enabled, On-Demand Instances use a lowest-price strategy, which launches the lowest-priced instances first. Spot Instances use a capacity-optimized strategy, which launches Spot Instances from pools that have optimal capacity for the number of instances that are launching. For both On-demand and spot, we recommend specifying a larger number of instance types to diversify and reduce the chance of experiencing insufficient capacity errors. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html#emr-instance-fleet-allocation-strategy BP 2.7 With instance fleet, diversify with instances in the same family and across generations first \u00b6 When deciding which instances to include in your instance fleet, it is recommend to first diversify across the same family. For example, if you are using m5.4xlarge, you should first add m5.8xlarge and then m5.12xlarge. Instances within the same family are identical and your job should perform consistent across the different instances. Ensure your application container (spark executors, tez container) is not larger than the smallest instance in your fleet. Next, you should diversify across generations, for example, including m6.4xlarge and m4.8xlarge. Diversifying your instance fleet across families should be considered last e.g r5 and m5 due to difference in core to memory ratios resulting in potential underutilization depending on your application container sizes. BP 2.8 With instance fleet, ensure the unit/weight matches the instance size or is proportional to the rest of the instances in your fleet \u00b6 When using instance fleets, you can specify multiple instance types and a total target capacity for your core or task fleet. When you specify an instance, you decide how much each instance counts toward the target. Ensure this unit/weight matches the actual instance size or is proportional to the rest of the instances in your fleet. For example, if your fleet includes: m5.2xlarge, m5.4xlarge and m5.8xlarge. You would want your units/weights to match the instance size - 2:4:8. This is to ensure that when EMR provision your cluster or scales up, you are consistently getting the same total compute. You could also do 1:2:4 since they are still proportional to the instance sizes. If the weights were not proportional, e.g 1:2:3, each time your cluster provisions, your total cluster capacity can be different. BP 2.9 If optimizing for availability, avoid exotic instance types \u00b6 Exotic instances are designed for specific use cases such as \u201czn\u201d, \u201cdn\u201c, and \u201cad\" as well as large instance types like 24xlarge. Exotic instance types have smaller EC2 capacity pools which increase the likelihood of Insufficient Capacity Errors and spot reclamation. It is recommended to avoid these types of instances if your use case does not have requirements for these types of instances and you want higher guarantees of instance availability. BP 2.10 When using auto scaling, keep core nodes constant and scale with only task nodes \u00b6 Scaling with only task nodes improves the time for nodes to scale in and out because task nodes do not coordinate storage as part of HDFS. As such, during scale up, task nodes do not need to install data node daemons and during scale down, task nodes do not need re balance HDFS blocks. Improvement in the time it takes to scale in and out improves performance and reduces cost. When scaling down with core nodes, you also risk saturating the remaining nodes disk volume during HDFS re balance. If the nodes disk utilization exceeds 90%, it\u2019ll mark the node as unhealthy making it unusable by YARN. In order to only scale with task nodes, you keep the number of core nodes constant and right size your core node EBS volumes for your HDFS usage. Remember to consider the hdfs replication factor which is configured via dfs.replication in hdfs-site.xml. It is recommended that a minimum, you keep 2 core nodes and set dfs.replication=2. Below is a managed scaling configuration example where the cluster will scale only on task nodes. In this example, the minimum nodes is 25, maximum 100. Of the 25 minimum, they will be all on-demand and core nodes. When the cluster needs to scale up, the remaining 75 will be task nodes on spot. BP 2.11 Handling S3 503 slow downs \u00b6 When you have an increased request rate to your S3 bucket, S3 might return 503 Slow Down errors while scaling to support the request rate. The default request rate is 3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket. There are a number of ways to handle S3 503 slow downs. 1) Use EMRFS retry strategies EMRFS provides 2 ways to improve the success rate of your S3 requests. You can adjust your retry strategy by configuring properties in your emrfs-site configuration. Increase the maximum retry limit for the default exponential back-off retry strategy. By default, the EMRFS retry limit is set to 4. You can increase the retry limit on a new cluster, on a running cluster, or at application runtime. (for example try 20-50 by setting fs.s3.maxRetries in emrfs-site.xml) Enable and configure the additive-increase/multiplicative-decrease (AIMD) retry strategy. AIMD is supported for Amazon EMR versions 6.4.0 and later. For more information, see: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-emrfs-retry.html 2) Increase fs.s3n.multipart.uploads.split.size Specifies the maximum size of a part, in bytes, before EMRFS starts a new part upload when multipart uploads is enabled. Default is 134217728 (134mb). The max is 5368709120 (5GB) \u2013 you can start with something in the middle and see if there\u2019s any impact to performance (for example 1-2 gb) For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-upload-s3.html#Config_Multipart 3) Combine or stagger out requests to S3 Combining requests to S3 reduces the number of calls per second. This can be achieved in a few ways: If the error happens during write, reduce the parallelism of the jobs. For example, use Spark .coalesce() or .repartition() operations to reduce number of Spark output partitions before writing to Amazon S3. You can also reduce the number of cores per executor or reduce the number of executors. If the error happens during read, compact small files in the source prefix. Compacting small files reduces the number of input files which reduces the number of Amazon S3 requests. If possible, stagger jobs out across the day or hour. For example, If your jobs don\u2019t all need to start at the same time or top of the hour, spread them across the hour or day to smoothen out the requests to S3. For more information, see: https://aws.amazon.com/premiumsupport/knowledge-center/emr-s3-503-slow-down/ 4) Optimize your S3 Data layout Rate limits (3,500 write and 5,500 read) are applied at the prefix level. By understanding your job access patterns, you can reduce throttling errors by partitioning your data in S3 For example, comparing the two s3 structures below, the second example with product in the prefix will allow you to achieve higher s3 request rates since requests are spread across different prefix. The S3 bucket limit would be 7,000 write requests and 11,000 read requests. s3://<bucket1>/dt=2021-11-01 s3://<bucket2>/product=1/dt=2021-11-01 s3://<bucket2>/product=2/dt=2021-11-01 It is also important that your S3 data layout is structured in a way that allows for partition pruning. With partition pruning, your applications will only scan the objects it needs and skip over the other prefixes reducing the number of requests to S3. For more information, see: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-performance.html#emr-spark-performance-dynamic BP 2.12 Audit and update EMR and EC2 limits to avoid throttling \u00b6 Amazon EMR throttles API calls to maintain system stability. EMR has two types of limits: 1) Limit on Resources - maximum number of clusters that can The maximum number of active clusters that can be run at the same time. The maximum number of active instances per instance group. 2) Limits on APIs Burst limit \u2013 This is the maximum number of API calls you can make at once. For example, the maximum number of AddInstanceFleet API requests that you can make per second is set at 5 calls/second as a default. This implies that the burst limit of AddInstanceFleet API is 5 calls/second, or that, at any given time, you can make at most 5 AddInstanceFleet API calls. However, after you use the burst limit, your subsequent calls are limited by the rate limit. Rate limit \u2013 This is the replenishment rate of the API's burst capacity. For example, replenishment rate of AddInstanceFleet calls is set at 0.5 calls/second as a default. This means that after you reach the burst limit, you have to wait at least 2 seconds (0.5 calls/second X 2 seconds = 1 call) to make the API call. If you make a call before that, you are throttled by the EMR web service. At any point, you can only make as many calls as the burst capacity without being throttled. Every additional second you wait, your burst capacity increases by 0.5 calls until it reaches the maximum limit of 5, which is the burst limit. To prevent throttling errors, we recommend: Reduce the frequency of the API calls. For example, if you\u2019re using the DescribeStep API and you don\u2019t need to know the status of the job right away, you can reduce the frequency of the call to 1min+ Stagger the intervals of the API calls so that they don't all run at the same time. Implement exponential back-off ( https://docs.aws.amazon.com/general/latest/gr/api-retries.html ) when making API calls. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-service-limits-what-are.html BP 2.13 Set dfs.replication > 1 if using Spot for core nodes or for long running clusters \u00b6 dfs.replication is the number of copies of each block to store for durability in HDFS. if dfs.replication is set to 1, and a Core node is lost due to spot reclamation or hardware failure, you risk losing HDFS data. Depending on the hdfs block that was lost, you may not be able to perform certain EMR actions. e.g submit hive job if core tez library in HDFS is missing dfs.replication defaults are set based off of initial core count: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hdfs-config.html To ensure the core node instance group is highly available, it is recommended that you launch at least two core nodes and set dfs.replication parameter to 2. Few other considerations: Do not scale down below dfs.replication. For example if dfs.replication=3, keep your core node minimum to 3 Increasing dfs.replication will require additional EBS volume BP 2.14 Right size your EBS volumes to avoid UNHEALTHY nodes \u00b6 When disk usage on a core or task node disk (for example, /mnt or /mnt1) exceeds 90%, the disk is marked as unhealthy. If fewer than 25% of a node's disks are healthy, the NodeManager marks the whole node as unhealthy and communicates this to the ResourceManager, which then stops assigning containers to the node. If the node remains UNHEALTHY for more than 45 minutes, YARN ResourceManager gracefully decommissions the node when termination protection is off. If termination protection is on, the core nodes remain in an UNHEALTHY state and only task nodes are terminated. The two most common reasons disk\u2019s exceed 90% are writing of HDFS and spark shuffle data. To avoid this scenario, it is recommended to right size your EBS volumes for your use case. You can either add more EBS volumes or increase the total size of the EBS capacity so that it never exceeds the default 90% utilization disk checker rate. From a monitoring and alerting perspective, there are a few options. You can monitor and alert on HDFS utilization using the Cloudwatch metric \u201cHDFSUtilization\u201d. This can help determine if disks are exceeding the 90% threshold due to HDFS usage. At a per node and disk level, using options in BP 1.12 can help identify if disk is filling due to spark shuffle or some other process. At a cluster level, you can also create an alarm for the MRUnhealthyNodes CloudWatch metric which reports the number of nodes reporting an UNHEALTHY status. Since UNHEALTHY nodes are excluded from processing tasks from YARN Resourcemanager, having UNHEALTHY nodes can degrade job performance. The 90% is a default value which can be configured by \u201cyarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage\" in yarn-site.xml. However, to fix nodes going UNHEALTHY, it is not recommended to adjust this % but instead, right size your EBS volumes. For more information, see: https://aws.amazon.com/premiumsupport/knowledge-center/emr-exit-status-100-lost-node/ https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_TerminationProtection.html Calculating required HDFS utilization: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html#emr-plan-instances-hdfs","title":"Best Practices"},{"location":"reliability/best_practices/#2-reliability","text":"Best Practices (BP) for running reliable workloads on EMR.","title":"2 - Reliability"},{"location":"reliability/best_practices/#bp-21-treat-all-clusters-as-transient-resources","text":"Whether you use your EMR cluster as a long or short running cluster, treat them as transient resources. This means you have the automation in place to re-provision clusters on demand and have standard templates to ensure cluster startup consistency. Even if you are using a long running clusters, it\u2019s recommended to recreate the cluster during some periodical interval. Services integrated with clusters also need to be decoupled from the cluster. For example any persistent data, meta data, scripts, and job/work orchestrator's (e.g oozie and airflow) should be stored off cluster. Decoupling the cluster from these services minimizes blast radius in the event of a cluster failure and non impacted clusters can continue using these off-cluster services. There are several benefits to this approach. It makes upgrading, patching, rotating AMI\u2019s or making any other infrastructure changes easier. It allows you to quickly recover from failures and it removes the operational overhead of managing a long running cluster. You may also see an improvement in cost since clusters will only run for the duration of your job or use case. If you need to store state on cluster, ensure the state is backed up and synced. For more information on orchestrating transient EMR cluster, see: https://aws.amazon.com/blogs/aws/new-using-step-functions-to-orchestrate-amazon-emr-workloads/ https://aws.amazon.com/blogs/big-data/orchestrating-analytics-jobs-on-amazon-emr-notebooks-using-amazon-mwaa/ https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-longrunning-transient.html Specifically for EMR application logging, consider using EMR\u2019s Persistent Application User Interfaces (Spark, YARN RM, Tez UI, etc) which are hosted by EMR off cluster and available even after clusters are terminated. For more information on off cluster monitoring options, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/app-history-spark-UI.html https://aws.amazon.com/blogs/big-data/monitor-and-optimize-analytic-workloads-on-amazon-emr-with-prometheus-and-grafana/ For more information on external catalog, see: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-metastore-external-hive.html","title":"BP 2.1 Treat all clusters as transient resources"},{"location":"reliability/best_practices/#bp-22-decouple-storage-and-compute","text":"Store persistent data in Amazon S3 and use the EMR File System (EMRFS) for reading and writing data from Amazon EMR. EMRFS is an implementation of HDFS that all Amazon EMR clusters use for accessing data in Amazon S3. Applications such as Apache Hive and Apache Spark work with Amazon S3 by mapping the HDFS APIs to Amazon S3 APIs (like EMRFS available with Amazon EMR). You specify which file system to use by the prefix of the URI used to access the data. For example, s3://DOC-EXAMPLE-BUCKET1/path references an Amazon S3 bucket using EMRFS. By keeping persistent data in Amazon S3, you minimize the impact that infrastructure or service disruptions can have on your data. For example, in the event of an EC2 hardware failure during an application run, data in Amazon S3 will not be impacted. You can provision a new cluster and re run your application that points to the existing S3 bucket. From an application and user perspective, by decoupling storage and compute, you can point many EMR clusters at the same source of truth. If you have different departments that want to operate different jobs, they can act in isolation without affecting the core production of your environment. This also allows you to split interactive query workloads with ETL type workloads which gives you more flexibility in how you operate For example, In an Amazon EMR environment you can provision a new cluster with a new technology and operate it in parallel on your data with your core production environment. Once you make a decision on which technology to adopt, you can easily cut over from one to other. This allows future proofing and option value because you can keep pace the analytic tool set evolves, your infrastructure can evolve with it, without any expensive re platforming or re transformation of data. HDFS is still available on Amazon EMR clusters and is a good option for temporary or intermediate data. For example, workloads with iterative reads on the same data set or Disk I/O intensive workloads. For example, some hive jobs write a lot of data to HDFS, either staging data or through a multi step pipeline. It may be more cost efficient and performant to use HDFS for these stages compared to writing to Amazon S3. You lose the HDFS data once EMR clusters are terminated so this should only be used for intermediate or staging data. Another strategy is to ensure that when using HDFS, you checkpoint data at regular intervals so that if you lose cluster mid-work, you do not have to restart from scratch. Once data is written to HDFS, you can use something like s3distcp to move your data to Amazon S3.","title":"BP 2.2 Decouple storage and compute"},{"location":"reliability/best_practices/#bp-23-use-the-latest-ami-and-emr-version-available","text":"In the Cost Optimization section, we talked about the benefits of using the latest EMR version. Equally important is using the latest AMI available. This ensures your up to date with the latest bug fixes, features and security updates. EMR allows has 2 AMI options available - default EMR AMI and Custom AMI. The default EMR AMI is based on the most up-to-date Amazon Linux AMI available at the time of the Amazon EMR release. Each Amazon EMR release version is \"locked\" to the Amazon Linux AMI version to maintain compatibility. This means that the same Amazon Linux AMI version is used for an Amazon EMR release version even when newer Amazon Linux AMIs become available. For this reason, we recommend that you use the latest Amazon EMR release version unless you need an earlier version for compatibility and are unable to migrate. When using a custom AMI, it is recommended to base your customization on the most recent EBS-backed Amazon Linux AMI (AL2 for 5.30.0 and later). Consider creating a new custom EMR AMI each time a new AL AMI is released. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-default-ami.html","title":"BP 2.3 Use the latest AMI and EMR version available"},{"location":"reliability/best_practices/#bp-24-spread-clusters-across-availability-zonessubnets-and-time-of-provisioning","text":"Spread clusters across multiple Availability Zones (AZ) to provide resiliency against AZ failures. An added benefit is that it can help reduce insufficient capacity errors (ICE) since your EC2 requests are now across multiple EC2 pools. Instances of a single cluster can only be provisioned in a single AZ. EMR helps you achieve this with instance fleets. Instead of specifying a single Amazon EC2 availability zone for your Amazon EMR cluster and a specific Amazon EC2 instance type for an Amazon EMR instance group, you can provide a list of availability zones and instances, and Amazon EMR will automatically select an optimal combination based on cost and availability. For example, If Amazon EMR detects an AWS large-scale event in one or more of the Availability Zones, or cannot get enough capacity, Amazon EMR automatically attempts to route traffic away from the impacted Availability Zones and tries to launch clusters in alternate Availability Zones according to your selections. With Instance Groups, you must explicitly set the subnet at provisioning time. You can still spread clusters across your AZs by picking through round robin or at random. If your use case allows, spread cluster provisioning times across the hour or day to distribute your requests to EC2 instead of provisioning clusters at the same time. This decreases the likelihood of getting insufficient capacity errors. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html","title":"BP 2.4 Spread clusters across availability zones/subnets and time of provisioning"},{"location":"reliability/best_practices/#bp-25-use-on-demand-for-core-nodes-and-spot-for-task","text":"Core nodes run the Data Node daemon to coordinate data storage as part of the Hadoop Distributed File System (HDFS). If a core node is running on spot and the spot node is reclaimed, Hadoop has to re balance the data in HDFS to the remaining core nodes. If there are no core nodes remaining, you run the risk of losing HDFS data and the name node going into safe mode making the cluster unhealthy and usable.","title":"BP 2.5 Use on demand for core nodes and spot for task"},{"location":"reliability/best_practices/#bp-26-use-instance-fleet-with-allocation-strategy","text":"The instance fleet configuration for Amazon EMR clusters lets you select a wide variety of provisioning options for Amazon EC2 instances, and helps you develop a flexible and elastic resourcing strategy for each node type in your cluster. You can have one instance fleet for each node group - master, core and task. Within the instance fleet, you specify a target capacity for on-demand and spot instances and with the allocation strategy option, you can select up to 30 instance types per fleet. In an instance fleet configuration, you specify a target capacity for On-Demand Instances and Spot Instances within each fleet. When the cluster launches, Amazon EMR provisions instances until the targets are fulfilled using any of the instances specified if your fleet. When Amazon EC2 reclaims a Spot Instance in a running cluster because of a price increase or instance failure, Amazon EMR tries to replace the instance with any of the instance types that you specify. This makes it easier to regain capacity during a spike in Spot pricing. It is recommended that you use the allocation strategy option for faster cluster provisioning, more accurate Spot Instance allocation, and fewer Spot Instance interruptions. With the allocation strategy enabled, On-Demand Instances use a lowest-price strategy, which launches the lowest-priced instances first. Spot Instances use a capacity-optimized strategy, which launches Spot Instances from pools that have optimal capacity for the number of instances that are launching. For both On-demand and spot, we recommend specifying a larger number of instance types to diversify and reduce the chance of experiencing insufficient capacity errors. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html#emr-instance-fleet-allocation-strategy","title":"BP 2.6 Use instance fleet with allocation strategy"},{"location":"reliability/best_practices/#bp-27-with-instance-fleet-diversify-with-instances-in-the-same-family-and-across-generations-first","text":"When deciding which instances to include in your instance fleet, it is recommend to first diversify across the same family. For example, if you are using m5.4xlarge, you should first add m5.8xlarge and then m5.12xlarge. Instances within the same family are identical and your job should perform consistent across the different instances. Ensure your application container (spark executors, tez container) is not larger than the smallest instance in your fleet. Next, you should diversify across generations, for example, including m6.4xlarge and m4.8xlarge. Diversifying your instance fleet across families should be considered last e.g r5 and m5 due to difference in core to memory ratios resulting in potential underutilization depending on your application container sizes.","title":"BP 2.7 With instance fleet, diversify with instances in the same family and across generations first"},{"location":"reliability/best_practices/#bp-28-with-instance-fleet-ensure-the-unitweight-matches-the-instance-size-or-is-proportional-to-the-rest-of-the-instances-in-your-fleet","text":"When using instance fleets, you can specify multiple instance types and a total target capacity for your core or task fleet. When you specify an instance, you decide how much each instance counts toward the target. Ensure this unit/weight matches the actual instance size or is proportional to the rest of the instances in your fleet. For example, if your fleet includes: m5.2xlarge, m5.4xlarge and m5.8xlarge. You would want your units/weights to match the instance size - 2:4:8. This is to ensure that when EMR provision your cluster or scales up, you are consistently getting the same total compute. You could also do 1:2:4 since they are still proportional to the instance sizes. If the weights were not proportional, e.g 1:2:3, each time your cluster provisions, your total cluster capacity can be different.","title":"BP 2.8 With instance fleet, ensure the unit/weight matches the instance size or is proportional to the rest of the instances in your fleet"},{"location":"reliability/best_practices/#bp-29-if-optimizing-for-availability-avoid-exotic-instance-types","text":"Exotic instances are designed for specific use cases such as \u201czn\u201d, \u201cdn\u201c, and \u201cad\" as well as large instance types like 24xlarge. Exotic instance types have smaller EC2 capacity pools which increase the likelihood of Insufficient Capacity Errors and spot reclamation. It is recommended to avoid these types of instances if your use case does not have requirements for these types of instances and you want higher guarantees of instance availability.","title":"BP 2.9 If optimizing for availability, avoid exotic instance types"},{"location":"reliability/best_practices/#bp-210-when-using-auto-scaling-keep-core-nodes-constant-and-scale-with-only-task-nodes","text":"Scaling with only task nodes improves the time for nodes to scale in and out because task nodes do not coordinate storage as part of HDFS. As such, during scale up, task nodes do not need to install data node daemons and during scale down, task nodes do not need re balance HDFS blocks. Improvement in the time it takes to scale in and out improves performance and reduces cost. When scaling down with core nodes, you also risk saturating the remaining nodes disk volume during HDFS re balance. If the nodes disk utilization exceeds 90%, it\u2019ll mark the node as unhealthy making it unusable by YARN. In order to only scale with task nodes, you keep the number of core nodes constant and right size your core node EBS volumes for your HDFS usage. Remember to consider the hdfs replication factor which is configured via dfs.replication in hdfs-site.xml. It is recommended that a minimum, you keep 2 core nodes and set dfs.replication=2. Below is a managed scaling configuration example where the cluster will scale only on task nodes. In this example, the minimum nodes is 25, maximum 100. Of the 25 minimum, they will be all on-demand and core nodes. When the cluster needs to scale up, the remaining 75 will be task nodes on spot.","title":"BP 2.10 When using auto scaling, keep core nodes constant and scale with only task nodes"},{"location":"reliability/best_practices/#bp-211-handling-s3-503-slow-downs","text":"When you have an increased request rate to your S3 bucket, S3 might return 503 Slow Down errors while scaling to support the request rate. The default request rate is 3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket. There are a number of ways to handle S3 503 slow downs. 1) Use EMRFS retry strategies EMRFS provides 2 ways to improve the success rate of your S3 requests. You can adjust your retry strategy by configuring properties in your emrfs-site configuration. Increase the maximum retry limit for the default exponential back-off retry strategy. By default, the EMRFS retry limit is set to 4. You can increase the retry limit on a new cluster, on a running cluster, or at application runtime. (for example try 20-50 by setting fs.s3.maxRetries in emrfs-site.xml) Enable and configure the additive-increase/multiplicative-decrease (AIMD) retry strategy. AIMD is supported for Amazon EMR versions 6.4.0 and later. For more information, see: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-emrfs-retry.html 2) Increase fs.s3n.multipart.uploads.split.size Specifies the maximum size of a part, in bytes, before EMRFS starts a new part upload when multipart uploads is enabled. Default is 134217728 (134mb). The max is 5368709120 (5GB) \u2013 you can start with something in the middle and see if there\u2019s any impact to performance (for example 1-2 gb) For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-upload-s3.html#Config_Multipart 3) Combine or stagger out requests to S3 Combining requests to S3 reduces the number of calls per second. This can be achieved in a few ways: If the error happens during write, reduce the parallelism of the jobs. For example, use Spark .coalesce() or .repartition() operations to reduce number of Spark output partitions before writing to Amazon S3. You can also reduce the number of cores per executor or reduce the number of executors. If the error happens during read, compact small files in the source prefix. Compacting small files reduces the number of input files which reduces the number of Amazon S3 requests. If possible, stagger jobs out across the day or hour. For example, If your jobs don\u2019t all need to start at the same time or top of the hour, spread them across the hour or day to smoothen out the requests to S3. For more information, see: https://aws.amazon.com/premiumsupport/knowledge-center/emr-s3-503-slow-down/ 4) Optimize your S3 Data layout Rate limits (3,500 write and 5,500 read) are applied at the prefix level. By understanding your job access patterns, you can reduce throttling errors by partitioning your data in S3 For example, comparing the two s3 structures below, the second example with product in the prefix will allow you to achieve higher s3 request rates since requests are spread across different prefix. The S3 bucket limit would be 7,000 write requests and 11,000 read requests. s3://<bucket1>/dt=2021-11-01 s3://<bucket2>/product=1/dt=2021-11-01 s3://<bucket2>/product=2/dt=2021-11-01 It is also important that your S3 data layout is structured in a way that allows for partition pruning. With partition pruning, your applications will only scan the objects it needs and skip over the other prefixes reducing the number of requests to S3. For more information, see: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-performance.html#emr-spark-performance-dynamic","title":"BP 2.11 Handling S3 503 slow downs"},{"location":"reliability/best_practices/#bp-212-audit-and-update-emr-and-ec2-limits-to-avoid-throttling","text":"Amazon EMR throttles API calls to maintain system stability. EMR has two types of limits: 1) Limit on Resources - maximum number of clusters that can The maximum number of active clusters that can be run at the same time. The maximum number of active instances per instance group. 2) Limits on APIs Burst limit \u2013 This is the maximum number of API calls you can make at once. For example, the maximum number of AddInstanceFleet API requests that you can make per second is set at 5 calls/second as a default. This implies that the burst limit of AddInstanceFleet API is 5 calls/second, or that, at any given time, you can make at most 5 AddInstanceFleet API calls. However, after you use the burst limit, your subsequent calls are limited by the rate limit. Rate limit \u2013 This is the replenishment rate of the API's burst capacity. For example, replenishment rate of AddInstanceFleet calls is set at 0.5 calls/second as a default. This means that after you reach the burst limit, you have to wait at least 2 seconds (0.5 calls/second X 2 seconds = 1 call) to make the API call. If you make a call before that, you are throttled by the EMR web service. At any point, you can only make as many calls as the burst capacity without being throttled. Every additional second you wait, your burst capacity increases by 0.5 calls until it reaches the maximum limit of 5, which is the burst limit. To prevent throttling errors, we recommend: Reduce the frequency of the API calls. For example, if you\u2019re using the DescribeStep API and you don\u2019t need to know the status of the job right away, you can reduce the frequency of the call to 1min+ Stagger the intervals of the API calls so that they don't all run at the same time. Implement exponential back-off ( https://docs.aws.amazon.com/general/latest/gr/api-retries.html ) when making API calls. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-service-limits-what-are.html","title":"BP 2.12 Audit and update  EMR and EC2 limits to avoid throttling"},{"location":"reliability/best_practices/#bp-213-set-dfsreplication-1-if-using-spot-for-core-nodes-or-for-long-running-clusters","text":"dfs.replication is the number of copies of each block to store for durability in HDFS. if dfs.replication is set to 1, and a Core node is lost due to spot reclamation or hardware failure, you risk losing HDFS data. Depending on the hdfs block that was lost, you may not be able to perform certain EMR actions. e.g submit hive job if core tez library in HDFS is missing dfs.replication defaults are set based off of initial core count: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hdfs-config.html To ensure the core node instance group is highly available, it is recommended that you launch at least two core nodes and set dfs.replication parameter to 2. Few other considerations: Do not scale down below dfs.replication. For example if dfs.replication=3, keep your core node minimum to 3 Increasing dfs.replication will require additional EBS volume","title":"BP 2.13 Set dfs.replication &gt; 1 if using Spot for core nodes or for long running clusters"},{"location":"reliability/best_practices/#bp-214-right-size-your-ebs-volumes-to-avoid-unhealthy-nodes","text":"When disk usage on a core or task node disk (for example, /mnt or /mnt1) exceeds 90%, the disk is marked as unhealthy. If fewer than 25% of a node's disks are healthy, the NodeManager marks the whole node as unhealthy and communicates this to the ResourceManager, which then stops assigning containers to the node. If the node remains UNHEALTHY for more than 45 minutes, YARN ResourceManager gracefully decommissions the node when termination protection is off. If termination protection is on, the core nodes remain in an UNHEALTHY state and only task nodes are terminated. The two most common reasons disk\u2019s exceed 90% are writing of HDFS and spark shuffle data. To avoid this scenario, it is recommended to right size your EBS volumes for your use case. You can either add more EBS volumes or increase the total size of the EBS capacity so that it never exceeds the default 90% utilization disk checker rate. From a monitoring and alerting perspective, there are a few options. You can monitor and alert on HDFS utilization using the Cloudwatch metric \u201cHDFSUtilization\u201d. This can help determine if disks are exceeding the 90% threshold due to HDFS usage. At a per node and disk level, using options in BP 1.12 can help identify if disk is filling due to spark shuffle or some other process. At a cluster level, you can also create an alarm for the MRUnhealthyNodes CloudWatch metric which reports the number of nodes reporting an UNHEALTHY status. Since UNHEALTHY nodes are excluded from processing tasks from YARN Resourcemanager, having UNHEALTHY nodes can degrade job performance. The 90% is a default value which can be configured by \u201cyarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage\" in yarn-site.xml. However, to fix nodes going UNHEALTHY, it is not recommended to adjust this % but instead, right size your EBS volumes. For more information, see: https://aws.amazon.com/premiumsupport/knowledge-center/emr-exit-status-100-lost-node/ https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_TerminationProtection.html Calculating required HDFS utilization: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html#emr-plan-instances-hdfs","title":"BP 2.14 Right size your EBS volumes to avoid UNHEALTHY nodes"},{"location":"reliability/introduction/","text":"Introduction \u00b6 EMR Reliability best practices covers the ability for workloads to recover from infrastructure or service disruptions, dynamically acquire computing resources to meet demand, imprve availability of resources when required and mitigate disruptions such as misconfiguration or transient network issues. It also includes the ability to understand the full travel path and change history of the data and keep the data safe when storage failure occurs. Contributing \u00b6 We encourage you to contribute to these guides. If you have implemented a practice that has proven to be effective, please share it with us by opening an issue or a pull request. Similarly, if you discover an error or flaw in the guidance we've already published, please submit a PR to correct it.","title":"Introduction"},{"location":"reliability/introduction/#introduction","text":"EMR Reliability best practices covers the ability for workloads to recover from infrastructure or service disruptions, dynamically acquire computing resources to meet demand, imprve availability of resources when required and mitigate disruptions such as misconfiguration or transient network issues. It also includes the ability to understand the full travel path and change history of the data and keep the data safe when storage failure occurs.","title":"Introduction"},{"location":"reliability/introduction/#contributing","text":"We encourage you to contribute to these guides. If you have implemented a practice that has proven to be effective, please share it with us by opening an issue or a pull request. Similarly, if you discover an error or flaw in the guidance we've already published, please submit a PR to correct it.","title":"Contributing"},{"location":"security/best_practices/","text":"Coming Soon","title":"Best Practices"},{"location":"security/introduction/","text":"Coming Soon","title":"Introduction"}]}