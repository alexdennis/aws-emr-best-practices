
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.2.3, mkdocs-material-8.1.11">
    
    
      
        <title>Best Practices - EMR Best Practices Guides</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.50e68009.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.e6a45f82.min.css">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#51-spark" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="EMR Best Practices Guides" class="md-header__button md-logo" aria-label="EMR Best Practices Guides" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            EMR Best Practices Guides
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Best Practices
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/aws/aws-emr-best-practices/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    aws/aws-emr-best-practices
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="EMR Best Practices Guides" class="md-nav__button md-logo" aria-label="EMR Best Practices Guides" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    EMR Best Practices Guides
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/aws/aws-emr-best-practices/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    aws/aws-emr-best-practices
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        Introduction
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          1 - Cost Optimizations
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="1 - Cost Optimizations" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          1 - Cost Optimizations
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../cost_optimization/introduction/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../cost_optimization/best_practices/" class="md-nav__link">
        Best Practices
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          2 - Reliability
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="2 - Reliability" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          2 - Reliability
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../reliability/introduction/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../reliability/best_practices/" class="md-nav__link">
        Best Practices
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          3 - Security
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="3 - Security" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          3 - Security
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../security/introduction/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../security/best_practices/" class="md-nav__link">
        Best Practices
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5">
          4 - Features
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="4 - Features" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          4 - Features
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../features/managed_scaling/best_practices/" class="md-nav__link">
        4.1 - Managed Scaling
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../features/spot_usage/best_practices/" class="md-nav__link">
        4.2 - Spot Usage
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6" type="checkbox" id="__nav_6" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_6">
          5 - Applications
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="5 - Applications" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          5 - Applications
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6_1" type="checkbox" id="__nav_6_1" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_6_1">
          5.1 Spark
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="5.1 Spark" data-md-level="2">
        <label class="md-nav__title" for="__nav_6_1">
          <span class="md-nav__icon md-icon"></span>
          5.1 Spark
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../introduction/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Best Practices
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Best Practices
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#bp-511-determine-right-infrastructure-for-your-spark-workloads" class="md-nav__link">
    BP 5.1.1  -  Determine right infrastructure for your Spark workloads
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-512-choose-the-right-deploy-mode" class="md-nav__link">
    BP 5.1.2  -  Choose the right deploy mode
  </a>
  
    <nav class="md-nav" aria-label="BP 5.1.2  -  Choose the right deploy mode">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#client-deploy-mode" class="md-nav__link">
    Client deploy mode
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cluster-deploy-mode" class="md-nav__link">
    Cluster deploy mode
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-513-use-right-file-formats-and-compression-type" class="md-nav__link">
    BP 5.1.3  -  Use right file formats and compression type
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-514-partitioning" class="md-nav__link">
    BP 5.1.4  -  Partitioning
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-515-tune-driverexecutor-memory-cores-and-sparksqlshufflepartitions-to-fully-utilize-cluster-resources" class="md-nav__link">
    BP 5.1.5 -  Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-516-use-kryo-serializer-by-registering-custom-classes-especially-for-dataset-schemas" class="md-nav__link">
    BP 5.1.6 -  Use Kryo serializer by registering custom classes especially for Dataset schemas
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-517-use-appropriate-garbage-collector" class="md-nav__link">
    BP 5.1.7  -   Use appropriate garbage collector
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-518-use-appropriate-apis-wherever-possible" class="md-nav__link">
    BP 5.1.8  -   Use appropriate APIs wherever possible
  </a>
  
    <nav class="md-nav" aria-label="BP 5.1.8  -   Use appropriate APIs wherever possible">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#repartition-vs-coalesce" class="md-nav__link">
    repartition vs coalesce
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#groupbykey-vs-reducebykey" class="md-nav__link">
    groupByKey vs reduceByKey
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#orderby-vs-sortby-or-sortwithinpartitions" class="md-nav__link">
    orderBy vs sortBy or sortWithinPartitions
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-519-leverage-spot-nodes-with-managed-autoscaling" class="md-nav__link">
    BP 5.1.9 -   Leverage spot nodes with managed autoscaling
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-5110-for-workloads-with-predictable-pattern-consider-disabling-dynamic-allocation" class="md-nav__link">
    BP 5.1.10  -   For workloads with predictable pattern, consider disabling dynamic allocation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-5111-leverage-hdfs-as-temporary-storage-for-io-intensive-workloads" class="md-nav__link">
    BP 5.1.11  -   Leverage HDFS as temporary storage for I/O intensive workloads
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-5112-spark-speculation-with-emrfs" class="md-nav__link">
    BP 5.1.12  -   Spark speculation with EMRFS
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-5113-data-quality-and-integrity-checks-with-deequ" class="md-nav__link">
    BP 5.1.13 -   Data quality and integrity checks with deequ
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-5114-use-dataframes-wherever-possible" class="md-nav__link">
    BP 5.1.14 -   Use DataFrames wherever possible
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-5115-data-skew" class="md-nav__link">
    BP 5.1.15  -   Data Skew
  </a>
  
    <nav class="md-nav" aria-label="BP 5.1.15  -   Data Skew">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#salting" class="md-nav__link">
    Salting
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#isolated-salting" class="md-nav__link">
    Isolated Salting
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#isolated-broadcast-join" class="md-nav__link">
    Isolated broadcast join
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hashing-for-sparksql-queries" class="md-nav__link">
    Hashing for SparkSQL queries
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-5116-use-right-type-of-join" class="md-nav__link">
    BP 5.1.16  -   Use right type of join
  </a>
  
    <nav class="md-nav" aria-label="BP 5.1.16  -   Use right type of join">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#broadcast-join" class="md-nav__link">
    Broadcast Join
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shuffle-hash-join" class="md-nav__link">
    Shuffle Hash Join
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sort-merge-join" class="md-nav__link">
    Sort Merge Join
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#broadcast-nested-loop-join" class="md-nav__link">
    Broadcast Nested Loop Join
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-5117-configuring-spark-executor-blacklist" class="md-nav__link">
    BP 5.1.17  - Configuring Spark Executor Blacklist
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-5118-configure-observability" class="md-nav__link">
    BP 5.1.18 -   Configure observability
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-5119-debugging-and-monitoring-spark-applications" class="md-nav__link">
    BP 5.1.19  - Debugging and monitoring Spark applications
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-5120-common-errors" class="md-nav__link">
    BP 5.1.20  -   Common Errors
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#bp-511-determine-right-infrastructure-for-your-spark-workloads" class="md-nav__link">
    BP 5.1.1  -  Determine right infrastructure for your Spark workloads
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-512-choose-the-right-deploy-mode" class="md-nav__link">
    BP 5.1.2  -  Choose the right deploy mode
  </a>
  
    <nav class="md-nav" aria-label="BP 5.1.2  -  Choose the right deploy mode">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#client-deploy-mode" class="md-nav__link">
    Client deploy mode
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cluster-deploy-mode" class="md-nav__link">
    Cluster deploy mode
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-513-use-right-file-formats-and-compression-type" class="md-nav__link">
    BP 5.1.3  -  Use right file formats and compression type
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-514-partitioning" class="md-nav__link">
    BP 5.1.4  -  Partitioning
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-515-tune-driverexecutor-memory-cores-and-sparksqlshufflepartitions-to-fully-utilize-cluster-resources" class="md-nav__link">
    BP 5.1.5 -  Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-516-use-kryo-serializer-by-registering-custom-classes-especially-for-dataset-schemas" class="md-nav__link">
    BP 5.1.6 -  Use Kryo serializer by registering custom classes especially for Dataset schemas
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-517-use-appropriate-garbage-collector" class="md-nav__link">
    BP 5.1.7  -   Use appropriate garbage collector
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-518-use-appropriate-apis-wherever-possible" class="md-nav__link">
    BP 5.1.8  -   Use appropriate APIs wherever possible
  </a>
  
    <nav class="md-nav" aria-label="BP 5.1.8  -   Use appropriate APIs wherever possible">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#repartition-vs-coalesce" class="md-nav__link">
    repartition vs coalesce
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#groupbykey-vs-reducebykey" class="md-nav__link">
    groupByKey vs reduceByKey
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#orderby-vs-sortby-or-sortwithinpartitions" class="md-nav__link">
    orderBy vs sortBy or sortWithinPartitions
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-519-leverage-spot-nodes-with-managed-autoscaling" class="md-nav__link">
    BP 5.1.9 -   Leverage spot nodes with managed autoscaling
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-5110-for-workloads-with-predictable-pattern-consider-disabling-dynamic-allocation" class="md-nav__link">
    BP 5.1.10  -   For workloads with predictable pattern, consider disabling dynamic allocation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-5111-leverage-hdfs-as-temporary-storage-for-io-intensive-workloads" class="md-nav__link">
    BP 5.1.11  -   Leverage HDFS as temporary storage for I/O intensive workloads
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-5112-spark-speculation-with-emrfs" class="md-nav__link">
    BP 5.1.12  -   Spark speculation with EMRFS
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-5113-data-quality-and-integrity-checks-with-deequ" class="md-nav__link">
    BP 5.1.13 -   Data quality and integrity checks with deequ
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-5114-use-dataframes-wherever-possible" class="md-nav__link">
    BP 5.1.14 -   Use DataFrames wherever possible
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-5115-data-skew" class="md-nav__link">
    BP 5.1.15  -   Data Skew
  </a>
  
    <nav class="md-nav" aria-label="BP 5.1.15  -   Data Skew">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#salting" class="md-nav__link">
    Salting
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#isolated-salting" class="md-nav__link">
    Isolated Salting
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#isolated-broadcast-join" class="md-nav__link">
    Isolated broadcast join
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hashing-for-sparksql-queries" class="md-nav__link">
    Hashing for SparkSQL queries
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-5116-use-right-type-of-join" class="md-nav__link">
    BP 5.1.16  -   Use right type of join
  </a>
  
    <nav class="md-nav" aria-label="BP 5.1.16  -   Use right type of join">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#broadcast-join" class="md-nav__link">
    Broadcast Join
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shuffle-hash-join" class="md-nav__link">
    Shuffle Hash Join
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sort-merge-join" class="md-nav__link">
    Sort Merge Join
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#broadcast-nested-loop-join" class="md-nav__link">
    Broadcast Nested Loop Join
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-5117-configuring-spark-executor-blacklist" class="md-nav__link">
    BP 5.1.17  - Configuring Spark Executor Blacklist
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-5118-configure-observability" class="md-nav__link">
    BP 5.1.18 -   Configure observability
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-5119-debugging-and-monitoring-spark-applications" class="md-nav__link">
    BP 5.1.19  - Debugging and monitoring Spark applications
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bp-5120-common-errors" class="md-nav__link">
    BP 5.1.20  -   Common Errors
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
<a href="https://github.com/aws/aws-emr-best-practices/edit/master/docs/applications/spark/best_practices.md" title="Edit this page" class="md-content__button md-icon">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
</a>


<h1 id="51-spark"><strong> 5.1 - Spark </strong><a class="headerlink" href="#51-spark" title="Permanent link">&para;</a></h1>
<h2 id="bp-511-determine-right-infrastructure-for-your-spark-workloads"><strong> BP 5.1.1  -  Determine right infrastructure for your Spark workloads </strong><a class="headerlink" href="#bp-511-determine-right-infrastructure-for-your-spark-workloads" title="Permanent link">&para;</a></h2>
<p>Spark workloads may require different types of hardware for different job characteristics to ensure optimal performance. EMR supports <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-supported-instance-types.html">several instance types</a> to cover all types of processing requirements. While onboarding new workloads, start your benchmarking with general instance types like m5s or m6gs. Monitor the OS and YARN metrics from Ganglia and CloudWatch to determine the system bottlenecks at peak load. Bottlenecks include CPU, memory, storage and I/O. Once identified, choose the appropriate hardware type for your job’s needs.</p>
<ul>
<li><strong>Memory-optimized</strong> instances like r5 and r4 are good candidates for memory intensive workloads. Spark jobs that cache large DataFrames, Datasets or RDDs, perform operations like joins and union on large tables, use a lot of internal or user-defined broadcast variables or accumulators, go through many GC cycles and perform massive shuffles (when dynamic allocation is disabled) are likely to be memory intensive. Following diagram shows YARN memory available percentage and aggregated OS memory utilization from Cloudwatch EMR namespace and Ganglia respectively.</li>
</ul>
<p><img alt="BP - 1" src="../images/spark-bp-1.png" /></p>
<ul>
<li><strong>CPU-optimized</strong> instances like c5 and c4 are good candidates for CPU intensive workloads . Spark jobs involving complex aggregate operations involving many in-built arithmetic functions or UDFs and jobs that use a lot of LRU caches are likely to be CPU intensive. Following screenshot shows aggregated CPU utilization of the EMR cluster from Ganglia.</li>
</ul>
<p><img alt="BP - 2" src="../images/spark-bp-2.png" /></p>
<ul>
<li><strong>General purpose</strong> instances like m5 and m4 are good candidates for a mix of CPU and memory intensive workloads. They are also great for benchmarking and onboarding your Spark applications. Following sheet outlines the CPU:Memory ratio of of 3 example instances instances at a similar price.</li>
</ul>
<table>
<thead>
<tr>
<th>Instance Type</th>
<th>Instance</th>
<th>EC2 price</th>
<th>EMR price</th>
<th>Cores</th>
<th>Memory in GiB</th>
<th>CPU:memory ratio</th>
</tr>
</thead>
<tbody>
<tr>
<td>Compute</td>
<td>m5.4xlarge</td>
<td>$3.06</td>
<td>0.27</td>
<td>72</td>
<td>144</td>
<td>2</td>
</tr>
<tr>
<td>Memory</td>
<td>m6g.4xlarge</td>
<td>$3.02</td>
<td>0.27</td>
<td>48</td>
<td>384</td>
<td>8</td>
</tr>
<tr>
<td>General</td>
<td>m5.16xlarge</td>
<td>$3.07</td>
<td>0.27</td>
<td>64</td>
<td>256</td>
<td>4</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>Storage-optimized</strong> instances like i3ens, d2 are good candidates for I/O intensive workloads. If your use case is CPU/memory bound but also consumes a lot of I/O, and demands high disk throughput and low read or write latencies from transient HDFS storage, you can consider using instances backed by SSD storage like r5ds, c5ds, m5ds etc.. For jobs that perform massive shuffles (when dynamic allocation is enabled), Spark external shuffle service will write the shuffle data blocks to the local disks of each node running executors.</li>
<li><strong>GPU instances</strong> such as p3 family for Spark ML and intensive analytical workloads like image processing. From EMR 6.2, you can also use <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-rapids.html">Nvidia RAPIDS accelerator</a> plugin to improve your GPU instance performance without any changes to your code or data processing pipelines.</li>
</ul>
<p>Starting EMR 5.31+ and 6.1+, there is Graviton support (r6g, m6g, c6g.) which offers up to 15% improvement in performance and 30% reduction in cost based on our derived benchmarks. They are a great choice to replace your legacy instances to observe improved performance at a lower cost.</p>
<p><img alt="BP - 3" src="../images/spark-bp-3.png" /></p>
<h2 id="bp-512-choose-the-right-deploy-mode"><strong> BP 5.1.2  -  Choose the right deploy mode </strong><a class="headerlink" href="#bp-512-choose-the-right-deploy-mode" title="Permanent link">&para;</a></h2>
<p>Spark offers two kinds of deploy modes called client and cluster deploy modes. Deploy mode determines where your Spark driver runs. Spark driver is the cockpit for your application. It hosts the SparkContext (or SparkSession) for your application. It keeps track of all tasks executed by the executors and the state of the executors via heartbeats. Driver also fetches the results from the executors running tasks. Choose the right deploy mode based on your workload requirements.</p>
<h3 id="client-deploy-mode">Client deploy mode<a class="headerlink" href="#client-deploy-mode" title="Permanent link">&para;</a></h3>
<p>This is the default deploy mode for Spark applications in EMR. In this deploy mode, the driver process will be launched within your Spark client - whether you submit your job from EMR master node (using EMR step or spark-submit) or using a client external to EMR. Spark driver is a single point of failure. A failed driver JVM will not be relaunched in client deploy mode. Also, when client loses connectivity with YARN, your job will be killed.</p>
<p><img alt="BP - 4" src="../images/spark-bp-4.png" /></p>
<p>Use this deploy mode if :-
* You are running only one or two Spark applications at a time in a cluster. This deploy mode is not ideal if you are running multiple applications at the same time on the same cluster since all those drivers running on a single node can lead to resource contention.
* You want to be more in control of your Spark driver configurations. In client mode, Spark driver resources will not compete with YARN resources and can be adjusted separately without affecting the resource procurement of your applications.
* If you are running too many executors (1000+). Since Spark driver manages and monitors tasks and executors, too many executors will slow down the driver since they have to send heartbeats, poll task status etc. Since EMR allows you to specify a different instance type for master instance, you can choose a very powerful instance like z1d and allocate the memory and CPU resources to Spark drivers especially if you are running a very high number of executors.
* If you want to print output to the console.</p>
<h3 id="cluster-deploy-mode">Cluster deploy mode<a class="headerlink" href="#cluster-deploy-mode" title="Permanent link">&para;</a></h3>
<p>In cluster deploy mode, your Spark driver will be colocated within the Application Master (AM) container from YARN regardless of where you submit your Spark application from.</p>
<p><img alt="BP - 5" src="../images/spark-bp-5.png" /></p>
<p>Use cluster deploy mode if :-
* You are submitting multiple applications at the same time or have higher job or EMR step concurrency. Since drivers will be launched within the AM, for multiple applications, the driver resources will be spread across the cluster considering AM will be launched on one of the worker nodes.
* If there are relatively fewer number of executors per application. i.e., the Spark driver process does not have to do intensive operations like manage and monitor from too many executors.
* You are storing results in S3/HDFS and there is no need to print output to the console.
* You want to specify detailed instructions on where AM runs. You can launch AMs on CORE partition or both CORE and TASK partitions based on where you want your AM and executors to launch. For example, you can run AM on only on-demand CORE nodes and executors only on spot task nodes.
* If you want to relaunch a failed driver JVM. By default, YARN re-attempts AM loss twice based on property "spark.yarn.maxAppAttempts". You can increase this value further if needed.
* If you want to ensure that termination of your client will not terminate your application.
* If you want to return success from your client after job submission based on the property "spark.yarn.submit.waitAppCompletion".</p>
<p>Regardless of which deploy mode you choose, make sure that your driver Spark configs are tuned for your workload needs.</p>
<h2 id="bp-513-use-right-file-formats-and-compression-type"><strong> BP 5.1.3  -  Use right file formats and compression type </strong><a class="headerlink" href="#bp-513-use-right-file-formats-and-compression-type" title="Permanent link">&para;</a></h2>
<p>It is highly recommended that you use right file formats for optimal performance. Do not use legacy file formats like CSV, JSON, text files etc. since the read/write performance will much slower. It is highly recommended that you use columnar file formats like Parquet, ORC etc. Especially for Spark, Parquet would be the best choice.</p>
<p>When using Parquet file format, Spark will use EMRFSOutputCommitter which is an optimized file committer for writing Parquet data files to S3 from Spark. Also, using Parquet file format is great for schema evolution, filter push downs and integration with applications with transactional support like Apache Hudi, Iceberg etc.</p>
<p>Also, it’s recommended to use an optimal compression format. Avoid using unsplittable compression formats like GZIP. Since they are not splittable, when there is a large GZIP compressed file, it can only be processed by a single task/executor leading to OOM errors.</p>
<p><img alt="BP - 6" src="../images/spark-bp-6.png" /></p>
<p>Use splittable compression formats like BZ2, LZO etc. Parquet uses Snappy compression by default. ORC uses ZLIB compression by default. Both compression types are good choices and you can continue to use defaults.</p>
<p><img alt="BP - 7" src="../images/spark-bp-7.png" /></p>
<p>You can also apply columnar encryption using KMS.</p>
<div class="codehilite"><pre><span></span><code><span class="n">sc</span><span class="o">.</span><span class="n">hadoopConfiguration</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;parquet.encryption.kms.client.class&quot;</span><span class="w"> </span><span class="p">,</span><span class="s2">&quot;org.apache.parquet.crypto.keytools.mocks.InMemoryKMS&quot;</span><span class="p">)</span><span class="w"></span>
<span class="o">//</span><span class="w"> </span><span class="n">Explicit</span><span class="w"> </span><span class="k">master</span><span class="w"> </span><span class="n">keys</span><span class="w"> </span><span class="p">(</span><span class="n">base64</span><span class="w"> </span><span class="n">encoded</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">required</span><span class="w"> </span><span class="n">only</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">mock</span><span class="w"> </span><span class="n">InMemoryKMS</span><span class="w"></span>
<span class="n">sc</span><span class="o">.</span><span class="n">hadoopConfiguration</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;parquet.encryption.key.list&quot;</span><span class="w"> </span><span class="p">,</span><span class="s2">&quot;keyA:AAECAwQFBgcICQoLDA0ODw== ,  keyB:AAECAAECAAECAAECAAECAA==&quot;</span><span class="p">)</span><span class="w"></span>
<span class="o">//</span><span class="w"> </span><span class="n">Activate</span><span class="w"> </span><span class="n">Parquet</span><span class="w"> </span><span class="n">encryption</span><span class="p">,</span><span class="w"> </span><span class="n">driven</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">Hadoop</span><span class="w"> </span><span class="n">properties</span><span class="w"></span>
<span class="n">sc</span><span class="o">.</span><span class="n">hadoopConfiguration</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;parquet.crypto.factory.class&quot;</span><span class="w"> </span><span class="p">,</span><span class="s2">&quot;org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory&quot;</span><span class="p">)</span><span class="w"></span>
<span class="o">//</span><span class="w"> </span><span class="n">Write</span><span class="w"> </span><span class="n">encrypted</span><span class="w"> </span><span class="n">dataframe</span><span class="w"> </span><span class="n">files</span><span class="o">.</span><span class="w"></span>
<span class="o">//</span><span class="w"> </span><span class="n">Column</span><span class="w"> </span><span class="s2">&quot;square&quot;</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">protected</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="k">master</span><span class="w"> </span><span class="n">key</span><span class="w"> </span><span class="s2">&quot;keyA&quot;</span><span class="o">.</span><span class="w"></span>
<span class="o">//</span><span class="w"> </span><span class="n">Parquet</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">footers</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">protected</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="k">master</span><span class="w"> </span><span class="n">key</span><span class="w"> </span><span class="s2">&quot;keyB&quot;</span><span class="w"></span>
<span class="n">squaresDF</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;parquet.encryption.column.keys&quot;</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;keyA:square&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;parquet.encryption.footer.key&quot;</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;keyB&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;/path/to/table.parquet.encrypted&quot;</span><span class="p">)</span><span class="w"></span>
<span class="o">//</span><span class="w"> </span><span class="n">Read</span><span class="w"> </span><span class="n">encrypted</span><span class="w"> </span><span class="n">dataframe</span><span class="w"> </span><span class="n">files</span><span class="w"></span>
<span class="n">val</span><span class="w"> </span><span class="n">df2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;/path/to/table.parquet.encrypted&quot;</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

<h2 id="bp-514-partitioning"><strong> BP 5.1.4  -  Partitioning </strong><a class="headerlink" href="#bp-514-partitioning" title="Permanent link">&para;</a></h2>
<p>Partitioning your data is very important if you are going to run your code or queries with filter conditions. Partitioning helps you arrange your data files into S3 prefixes based on the partition key. It helps minimize read/write access footprint i.e., you will be able to only read files from partition folder specified in your where clause - thus skipping a full read. Partitioning can also help if your data ingestion is incremental in nature. However, partitioning can reduce read throughput if you are performing full table scans.</p>
<p>You can choose your partition field based on :-</p>
<ul>
<li>Query pattern. i.e., if you find workloads use 1-2 columns frequently as filter fields more so than other columns, it is recommended to consider using them as partitioning field.</li>
<li>Ingestion pattern. i.e., if you load data into your table once everyday, if you want to avoid re-writing historical data, you can partition your data based on date field (typically in YYYY-MM-DD format)</li>
<li>Cardinality of the partitioning column. For partitioning, cardinality should not be too high.</li>
<li>File sizes per partition. It is highly recommended that your individual file sizes within each partition is ~128 MB and not too small since that would be optimal for Spark executor JVM to process.</li>
</ul>
<p>Also, number of shuffle partitions will determine the number of output files per partition.</p>
<div class="codehilite"><pre><span></span><code>df.repartition(400).write.partitionBy(&quot;datecol&quot;).parquet(&quot;s3://bucket/output/&quot;)
</code></pre></div>

<p>The above code will create maximum of 400 files per datecol partition. Repartition API alters number of shuffle partitions during runtime and partitionBy API determines the partition field(s). You can also control the number of shuffle partitions with the Spark property "spark.sql.shuffle.partitions". You can use repartition API to control the file size in destination. i.e., for merging smaller files. But for splitting large files, you can use the property "spark.sql.files.maxPartitionBytes".</p>
<p>Partitioning ensures that pruning takes place during reads and writes. Pruning makes sure that only necessary partition(s) are read from S3 or HDFS. Spark logical plan or DAG can be studied to ensure that pruning takes place while reading and writing to partitioned tables from Spark. It may look similar to below.</p>
<p>FileScan parquet default.dimension[Store#12,Date#13,Price#14,CPI#15] Batched: true, DataFilters: [isnotnull(Date#16), (Date#16 &gt; 02–14–2022), isnotnull(Store#12)], Format: Parquet, Location: InMemoryFileIndex[dbfs:/user/hive/warehouse/dimension], PartitionFilters: [], <strong>PushedFilters: [IsNotNull(Date), GreaterThan(Date,02–14–2022), IsNotNull(Store)]</strong>, ReadSchema: struct<Store:int,Date:string,Price:double,CPI:string></p>
<p>FileScan parquet default.fact[Dept#22,Sales#23,Store#24,Date#25] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[dbfs:/user/hive/warehouse/fact/Store=12/Date=02–14–2022…, PartitionFilters: [isnotnull(Date#27), (Date#25 &gt; 02–14–2022), isnotnull(Store#24), <strong>dynamicpruningexpression(Date#2…, PushedFilters: [], ReadSchema: struct<Dept:int,Sales:double></strong></p>
<h2 id="bp-515-tune-driverexecutor-memory-cores-and-sparksqlshufflepartitions-to-fully-utilize-cluster-resources"><strong> BP 5.1.5 -  Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources </strong><a class="headerlink" href="#bp-515-tune-driverexecutor-memory-cores-and-sparksqlshufflepartitions-to-fully-utilize-cluster-resources" title="Permanent link">&para;</a></h2>
<p>Amazon EMR configures Spark defaults during the cluster launch based on your cluster's infrastructure (number of instances and instance types). EMR configured defaults are generally sufficient for majority of the workloads. However, if it is not meeting your performance expectations, we recommend you to tune Spark driver/executor configurations and see if you can achieve a better performance. Following are the general recommendations.  </p>
<p>For a starting point, generally, its advisable to set spark.executor.cores to 4 or 5 and tune spark.executor.memory around this value. Also, when you calculate the spark.executor.memory, you need to account for the executor overhead which is set to 0.1875 by default (i.e., 18.75% of spark.executor.memory). For example, for a 2 worker node r4.8xlarge cluster, following will be the configurations.</p>
<p>Based on <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hadoop-task-config.html">Task Configurations</a> r4.8xlarge node has YARN memory of 241664 MB (based on the value of yarn.nodemanager.resource.memory-mb). The instance has 32 vCores. If we set spark.executor.cores as 4, we can run 8 executors at a time. So, the configurations will be following.</p>
<p>spark.executor.cores = 4
spark.executor.memory + (spark.executor.memory * spark.yarn.executor.memoryOverheadFactor) = (241664 MB / 8) = 30208 MB
spark.executor.memory = 24544 MB (substituting default spark.yarn.executor.memoryOverheadFactor=0.1875)</p>
<p>If you have a cluster of 10 r4.8xlarge nodes, then totally, 80 executors can run with 24544 MB memory and 4 vCores each.</p>
<p>Please note that some jobs benefit from bigger executor JVMs (more cores assigned). Some jobs benefit from smaller but more number of executors. So, you can use the above formula to arrive at optimal values for your application. EMR Spark has a feature called <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html#emr-spark-maximizeresourceallocation">maximizeResourceAllocation</a>. Setting this property to true will lead to one fat JVM per node that uses all of the available cores in that instance. However, please note that this setting may not prove to be optimal all types of workloads. It is not recommended to set this property to true if your cluster is a shared cluster with multiple parallel applications or if your cluster has HBase.</p>
<p>After configuring the values, run a sample job and monitor the Resource Manager UI, ContainerPendingRatio and YARNMemoryAvailablePcnt Cloudwatch metrics to verify that the vCores and YARN memory are being fully utilized. <a href="https://spark.apache.org/docs/latest/monitoring.html">Spark JMX metrics</a> provides JMX level visibility which is the best way to determine resource utilization.</p>
<p>While using instance fleets, it is generally advisable to request worker nodes with similar vCore:memory ratio (for eg: requesting r4, r5 and r6gs in the same fleet). However, in some cases, in order to ensure capacity, you may have to diversify the instance type families as well in your request (for eg: requesting c5s, m5s and r5s in same fleet). EMR will configure driver/executor memory based on minimum of master, core and task OS memory. Generally, in this case, it is best to use the default configurations. However, if needed, you can fine tune the driver and executor configurations based on above principles. But in that case, you will need to take YARN memory and vCores of all the different instance families into consideration.</p>
<p>To provide an example, lets say you have requested a cluster with a core fleet containing following instances: r5.4xlarge, r5.12xlarge, c5.4xlarge, c5.12xlarge, m5.4xlarge, m5.12xlarge. All the 4xlarge instances in this fleet have 16 vCores and 12xlarge instances have 48 vCores. But the OS/YARN memory are different.</p>
<table>
<thead>
<tr>
<th>Instance</th>
<th>YARN memory in MB</th>
</tr>
</thead>
<tbody>
<tr>
<td>c5.4xlarge</td>
<td>24576</td>
</tr>
<tr>
<td>c5.12xlarge</td>
<td>90112</td>
</tr>
<tr>
<td>m5.4xlarge</td>
<td>57344</td>
</tr>
<tr>
<td>m5.12xlarge</td>
<td>188416</td>
</tr>
<tr>
<td>r5.4xlarge</td>
<td>122880</td>
</tr>
<tr>
<td>r5.12xlarge</td>
<td>385024</td>
</tr>
</tbody>
</table>
<p>Now, let us calculate executor memory after setting spark.executor.cores = 4 by starting with smallest YARN memory from the above table (c5.4xlarge) and dividing the YARN memory by spark.executor.cores to get the total container size -&gt; 24576 / 4 = 6144.</p>
<p>spark.executor.memory = 6144 - (6144 * 0.1875) = 4992 MB</p>
<p>In default Spark implementation, with the above math, if you set 4992 MB as executor memory, then in r5.12xlarge, the resources will be under utilized even though you will not see the evidence of it from the Resource Manager UI. With the above configs, 77 executors can run in r5.12xlarge but there are only 48 vCores. So, even though 77 executors will have YARN resources allocated, they are only able to run 48 tasks at any given time which could be considered a wastage of JVMs.</p>
<p>In order to alleviate this issue, from EMR 5.32 and EMR 6.2, there is a feature called Heterogenous Executors which dynamically calculates executor sizes. It is defined by the property "spark.yarn.heterogeneousExecutors.enabled" and is set to "true" by default. Further, you will be able to control the maximum resources allocated to each executor with properties "spark.executor.maxMemory" and "spark.executor.maxCores". Minimum resources are calculated with "spark.executor.cores" and "spark.executor.memory". For uniform instance groups or for flexible fleets with instance types having similar vCore:memory ratio, you can try setting this property to false and see if you get better performance.</p>
<p>Similar to executors, driver memory and vCores can be calculated as well. The default memory overhead for driver container is 10% of driver memory. If you are using cluster deploy mode, then the driver resources will be allocated from one of the worker nodes. So, based on the driver memory/core configurations, it will take away some of the YARN resources that could be used for launching executors. If you are using client deploy mode and submitting jobs from EMR master node or a remote server, then driver resources are taken from the master node or remote server and will not affect the resources available for executor JVMs. The default driver memory (without maximizeResourceAllocation) is 2 GB. You can increase driver memory or cores for following conditions:</p>
<ol>
<li>Your cluster size is very large and there are many executors (1000+) that need to send heartbeats to driver.</li>
<li>Your result size retrieved during actions such as printing output to console is very large. For this, you will also need to tune "spark.driver.maxResultSize".</li>
</ol>
<p>You can use smaller driver memory (or use the default spark.driver.memory) if you are running multiple jobs in parallel.</p>
<p>Now, coming to "spark.sql.shuffle.partitions" for Dataframes and Datasets and "spark.default.parallelism" for RDDs, it is recommended to set this value to total number of vCores in your cluster or a multiple of that value. For example, a 10 core node r4.8xlarge cluster can accommodate 320 vCores in total. Hence, you can set shuffle partitions or parallelism to 320 or a multiple of 320 such that each vCore handles a Spark partition. It is not recommended to set this value too high or too low. Generally 1 or 2x the total number of vCores is optimal. Generally, each Spark shuffle partition should process ~128 MB of data. This can be determined by looking at the execution plan from the Spark UI.</p>
<p><img alt="BP - 16" src="../images/spark-bp-16.png" /></p>
<p>From the above screenshot, you can see average size in exchange is 2.2 KB which means we can try to reduce "spark.sql.shuffle.partitions" to increase partition size during exchange (or shuffle).</p>
<p>Apart from this, if you want to use a tooling for configuration suggestions, consider using <a href="https://aws.amazon.com/blogs/big-data/tune-hadoop-and-spark-performance-with-dr-elephant-and-sparklens-on-amazon-emr/">Sparklens and Dr. Elephant</a> with Amazon EMR which will provide tuning suggestions based on metrics collected during your job run.</p>
<h2 id="bp-516-use-kryo-serializer-by-registering-custom-classes-especially-for-dataset-schemas"><strong> BP 5.1.6 -  Use Kryo serializer by registering custom classes especially for Dataset schemas </strong><a class="headerlink" href="#bp-516-use-kryo-serializer-by-registering-custom-classes-especially-for-dataset-schemas" title="Permanent link">&para;</a></h2>
<p>Spark uses Java Serializer by default. From Spark 2.0+, Spark internally uses Kryo Serializer when shuffling RDDs with simple types, arrays of simple types, or string type. It is highly recommended that you use Kryo Serializer and also register your classes in the application.</p>
<div class="codehilite"><pre><span></span><code><span class="n">val</span><span class="w"> </span><span class="n">spark</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">SparkSession</span><span class="w"></span>
<span class="w">                  </span><span class="p">.</span><span class="n">builder</span><span class="w"></span>
<span class="w">                  </span><span class="p">.</span><span class="n">appName</span><span class="p">(</span><span class="s">&quot;my spark application name&quot;</span><span class="p">)</span><span class="w"></span>
<span class="w">                  </span><span class="p">.</span><span class="n">config</span><span class="p">(</span><span class="n">getConfig</span><span class="p">)</span><span class="w"></span>
<span class="w">                  </span><span class="p">.</span><span class="n">config</span><span class="p">(</span><span class="s">&quot;spark.serializer&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;org.apache.spark.serializer.KryoSerializer&quot;</span><span class="p">)</span><span class="w"></span>
<span class="w">                  </span><span class="c1">// use this if you need to increment Kryo buffer size. Default 64k</span>
<span class="w">                  </span><span class="p">.</span><span class="n">config</span><span class="p">(</span><span class="s">&quot;spark.kryoserializer.buffer&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;1024k&quot;</span><span class="p">)</span><span class="w"></span>
<span class="w">                  </span><span class="c1">// use this if you need to increment Kryo buffer max size. Default 64m</span>
<span class="w">                  </span><span class="p">.</span><span class="n">config</span><span class="p">(</span><span class="s">&quot;spark.kryoserializer.buffer.max&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;1024m&quot;</span><span class="p">)</span><span class="w"></span>
<span class="w">                  </span><span class="cm">/*</span>
<span class="cm">                  * Use this if you need to register all Kryo required classes.</span>
<span class="cm">                  * If it is false, you do not need register any class for Kryo, but it will increase your data size when the data is serializing.</span>
<span class="cm">                  */</span><span class="w"></span>
<span class="w">                  </span><span class="p">.</span><span class="n">config</span><span class="p">(</span><span class="s">&quot;spark.kryo.registrationRequired&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;true&quot;</span><span class="p">)</span><span class="w"></span>
<span class="w">                  </span><span class="p">.</span><span class="n">getOrCreate</span><span class="w"></span>
</code></pre></div>

<p>If you do not specify classesToRegister, then there will be a Kryo conversion overhead which could impact performance. Hence, it is highly recommended to register classes in your application. Especially, if you are using Datasets, consider registering your Dataset schema classes along with classes used in Spark internally based on the data types and structures used in your program. An example provided below:</p>
<div class="codehilite"><pre><span></span><code><span class="n">val</span><span class="w"> </span><span class="n">conf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">SparkConf</span><span class="p">()</span><span class="w"></span>
<span class="w">        </span><span class="n">conf</span><span class="p">.</span><span class="n">registerKryoClasses</span><span class="p">(</span><span class="w"></span>
<span class="w">          </span><span class="k">Array</span><span class="p">(</span><span class="w"></span>
<span class="w">            </span><span class="n">classOf</span><span class="o">[</span><span class="n">org.myPackage.FlightDataset</span><span class="o">]</span><span class="p">,</span><span class="w"></span>
<span class="w">            </span><span class="n">classOf</span><span class="o">[</span><span class="n">org.myPackage.BookingDataset</span><span class="o">]</span><span class="p">,</span><span class="w"></span>
<span class="w">            </span><span class="n">classOf</span><span class="o">[</span><span class="n">scala.collection.mutable.WrappedArray.ofRef[_</span><span class="o">]</span><span class="err">]</span><span class="p">,</span><span class="w"></span>
<span class="w">            </span><span class="n">classOf</span><span class="o">[</span><span class="n">org.apache.spark.sql.types.StructType</span><span class="o">]</span><span class="p">,</span><span class="w"></span>
<span class="w">            </span><span class="n">classOf</span><span class="o">[</span><span class="n">Array[org.apache.spark.sql.types.StructType</span><span class="o">]</span><span class="err">]</span><span class="p">,</span><span class="w"></span>
<span class="w">            </span><span class="n">classOf</span><span class="o">[</span><span class="n">org.apache.spark.sql.types.StructField</span><span class="o">]</span><span class="p">,</span><span class="w"></span>
<span class="w">            </span><span class="n">classOf</span><span class="o">[</span><span class="n">Array[org.apache.spark.sql.types.StructField</span><span class="o">]</span><span class="err">]</span><span class="p">,</span><span class="w"></span>
<span class="w">            </span><span class="k">Class</span><span class="p">.</span><span class="n">forName</span><span class="p">(</span><span class="ss">&quot;org.apache.spark.sql.types.StringType$&quot;</span><span class="p">),</span><span class="w"></span>
<span class="w">            </span><span class="k">Class</span><span class="p">.</span><span class="n">forName</span><span class="p">(</span><span class="ss">&quot;org.apache.spark.sql.types.LongType$&quot;</span><span class="p">),</span><span class="w"></span>
<span class="w">            </span><span class="k">Class</span><span class="p">.</span><span class="n">forName</span><span class="p">(</span><span class="ss">&quot;org.apache.spark.sql.types.BooleanType$&quot;</span><span class="p">),</span><span class="w"></span>
<span class="w">            </span><span class="k">Class</span><span class="p">.</span><span class="n">forName</span><span class="p">(</span><span class="ss">&quot;org.apache.spark.sql.types.DoubleType$&quot;</span><span class="p">),</span><span class="w"></span>
<span class="w">            </span><span class="n">classOf</span><span class="o">[</span><span class="n">org.apache.spark.sql.types.Metadata</span><span class="o">]</span><span class="p">,</span><span class="w"></span>
<span class="w">            </span><span class="n">classOf</span><span class="o">[</span><span class="n">org.apache.spark.sql.types.ArrayType</span><span class="o">]</span><span class="p">,</span><span class="w"></span>
<span class="w">            </span><span class="k">Class</span><span class="p">.</span><span class="n">forName</span><span class="p">(</span><span class="ss">&quot;org.apache.spark.sql.execution.joins.UnsafeHashedRelation&quot;</span><span class="p">),</span><span class="w"></span>
<span class="w">            </span><span class="n">classOf</span><span class="o">[</span><span class="n">org.apache.spark.sql.catalyst.InternalRow</span><span class="o">]</span><span class="p">,</span><span class="w"></span>
<span class="w">            </span><span class="n">classOf</span><span class="o">[</span><span class="n">Array[org.apache.spark.sql.catalyst.InternalRow</span><span class="o">]</span><span class="err">]</span><span class="p">,</span><span class="w"></span>
<span class="w">            </span><span class="n">classOf</span><span class="o">[</span><span class="n">org.apache.spark.sql.catalyst.expressions.UnsafeRow</span><span class="o">]</span><span class="p">,</span><span class="w"></span>
<span class="w">            </span><span class="k">Class</span><span class="p">.</span><span class="n">forName</span><span class="p">(</span><span class="ss">&quot;org.apache.spark.sql.execution.joins.LongHashedRelation&quot;</span><span class="p">),</span><span class="w"></span>
<span class="w">            </span><span class="k">Class</span><span class="p">.</span><span class="n">forName</span><span class="p">(</span><span class="ss">&quot;org.apache.spark.sql.execution.joins.LongToUnsafeRowMap&quot;</span><span class="p">),</span><span class="w"></span>
<span class="w">            </span><span class="n">classOf</span><span class="o">[</span><span class="n">org.apache.spark.util.collection.BitSet</span><span class="o">]</span><span class="p">,</span><span class="w"></span>
<span class="w">            </span><span class="n">classOf</span><span class="o">[</span><span class="n">org.apache.spark.sql.types.DataType</span><span class="o">]</span><span class="p">,</span><span class="w"></span>
<span class="w">            </span><span class="n">classOf</span><span class="o">[</span><span class="n">Array[org.apache.spark.sql.types.DataType</span><span class="o">]</span><span class="err">]</span><span class="p">,</span><span class="w"></span>
<span class="w">            </span><span class="k">Class</span><span class="p">.</span><span class="n">forName</span><span class="p">(</span><span class="ss">&quot;org.apache.spark.sql.types.NullType$&quot;</span><span class="p">),</span><span class="w"></span>
<span class="w">            </span><span class="k">Class</span><span class="p">.</span><span class="n">forName</span><span class="p">(</span><span class="ss">&quot;org.apache.spark.sql.types.IntegerType$&quot;</span><span class="p">),</span><span class="w"></span>
<span class="w">            </span><span class="k">Class</span><span class="p">.</span><span class="n">forName</span><span class="p">(</span><span class="ss">&quot;org.apache.spark.sql.types.TimestampType$&quot;</span><span class="p">),</span><span class="w"></span>
<span class="w">            </span><span class="k">Class</span><span class="p">.</span><span class="n">forName</span><span class="p">(</span><span class="ss">&quot;org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteTaskResult&quot;</span><span class="p">),</span><span class="w"></span>
<span class="w">            </span><span class="k">Class</span><span class="p">.</span><span class="n">forName</span><span class="p">(</span><span class="ss">&quot;org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage&quot;</span><span class="p">),</span><span class="w"></span>
<span class="w">            </span><span class="k">Class</span><span class="p">.</span><span class="n">forName</span><span class="p">(</span><span class="ss">&quot;scala.collection.immutable.Set$EmptySet$&quot;</span><span class="p">),</span><span class="w"></span>
<span class="w">            </span><span class="k">Class</span><span class="p">.</span><span class="n">forName</span><span class="p">(</span><span class="ss">&quot;scala.reflect.ClassTag$$anon$1&quot;</span><span class="p">),</span><span class="w"></span>
<span class="w">            </span><span class="k">Class</span><span class="p">.</span><span class="n">forName</span><span class="p">(</span><span class="ss">&quot;java.lang.Class&quot;</span><span class="p">)</span><span class="w"></span>
<span class="w">          </span><span class="p">)</span><span class="w"></span>
<span class="w">        </span><span class="p">)</span><span class="w"></span>
<span class="w">    </span><span class="err">}</span><span class="w"></span>
</code></pre></div>

<p>You can also fine tune the following Kryo configs :-</p>
<p><strong>spark.kryo.unsafe</strong> - Set to false for faster serialization. This is not unsafer for same platforms but should not be used if your EMR cluster has a mix of AMD and intel types for example.
<strong>spark.kryoserializer.buffer.max</strong> - Maximum size of Kryo buffer. Default is 64m. Recommended to increase but this property upto 1024m value should be below 2048m
<strong>spark.kryoserializer.buffer</strong> - Initial size of Kryo's serialization buffer. Default is 64k. Recommended to increase up to 1024k.</p>
<h2 id="bp-517-use-appropriate-garbage-collector"><strong> BP 5.1.7  -   Use appropriate garbage collector </strong><a class="headerlink" href="#bp-517-use-appropriate-garbage-collector" title="Permanent link">&para;</a></h2>
<p>By default, EMR Spark uses Parallel Garbage Collector which works well in most cases. You can change the GC to G1GC if your GC cycles are slow since G1GC may provide better performance in some cases specifically by reducing GC pause times. Also, since G1GC is the default garbage collector since Java 9, you may want to switch to G1GC for forward compatibility.</p>
<p>Following is the spark configuration :-</p>
<div class="codehilite"><pre><span></span><code>[{
&quot;classification&quot;: &quot;spark-defaults&quot;,
&quot;properties&quot;: {
    &quot;spark.executor.extraJavaOptions&quot;: &quot;-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError=&#39;kill -9 %p&#39;&quot;,
    &quot;spark.driver.extraJavaOptions&quot;: &quot;-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError=&#39;kill -9 %p&#39;&quot;
},
&quot;configurations&quot;: []
}]
</code></pre></div>

<p>You can also tune the GC parameters for GC performance. You can see the comprehensive list of parameters <a href="https://www.oracle.com/technical-resources/articles/java/g1gc.html">here</a> for G1GC and <a href="https://docs.oracle.com/en/java/javase/11/gctuning/parallel-collector1.html">here</a> for ParallelGC. Some useful ones are below :-</p>
<p>-XX:ConcGCThreads=n
-XX:ParallelGCThreads=n
-XX:InitiatingHeapOccupancyPercent=45
-XX:MaxGCPauseMillis=200</p>
<p>You can also monitor GC performance using Spark UI. the GC time should be ideally &lt;= 1% of total task runtime. If not, tune the GC settings or executor size. For example, we see below in the Spark UI that GC takes almost 25% of task runtime which is a poor GC performance.</p>
<p><img alt="BP - 8" src="../images/spark-bp-8.png" /></p>
<h2 id="bp-518-use-appropriate-apis-wherever-possible"><strong> BP 5.1.8  -   Use appropriate APIs wherever possible </strong><a class="headerlink" href="#bp-518-use-appropriate-apis-wherever-possible" title="Permanent link">&para;</a></h2>
<p>When using spark APIs, try to go with the most optimal choice if your use case permits. Following are a few examples.</p>
<h3 id="repartition-vs-coalesce">repartition vs coalesce<a class="headerlink" href="#repartition-vs-coalesce" title="Permanent link">&para;</a></h3>
<p>Both repartition and coalesce are used for changing the number of shuffle partitions. Repartition is used for both increasing and decreasing the shuffle partitions whereas coalesce is used for only decreasing the number of shuffle partitions. If your goal is to decrease the number of shuffle partitions, consider using coalesce instead of repartition. The reason is, repartition triggers a full shuffle but coalesce triggers only a partial shuffle and thus minimizes the amount of data shuffled by keeping a few nodes as receivers of shuffle data.</p>
<div class="codehilite"><pre><span></span><code>df.coalesce(1) //instead of df.repartition(1)
</code></pre></div>

<p>But please note that when you coalesce (or repartition) to a very small number, your JVM will process a lot of data which can lead to OOM issues or disk space issues due to shuffle spill.</p>
<h3 id="groupbykey-vs-reducebykey">groupByKey vs reduceByKey<a class="headerlink" href="#groupbykey-vs-reducebykey" title="Permanent link">&para;</a></h3>
<p>Use reduceByKey instead of groupByKey wherever possible. With groupByKey, data will be transferred over the network and collected on the reduced workers. This can lead to OOMs since all data is sent across the network. Whereas, with reduceByKey, data is combined at partition-level, with only one output for one key at each partition to send over the network. reduceByKey required combining all your values into another value with the exact same type.</p>
<h3 id="orderby-vs-sortby-or-sortwithinpartitions">orderBy vs sortBy or sortWithinPartitions<a class="headerlink" href="#orderby-vs-sortby-or-sortwithinpartitions" title="Permanent link">&para;</a></h3>
<p>orderBy does global sorting. i.e., all data is sorted in a single JVM. Whereas, sortBy or sortWithinPartitions does local sorting i.e., data is sorted within each partition but it does not preserve global ordering. Use sortBy or sortWithinPartitions if global sorting is not necessary - especially during writes. Try to avoid orderBy clause. Values can be aggregated across partitions in your queries if needed.</p>
<h2 id="bp-519-leverage-spot-nodes-with-managed-autoscaling"><strong> BP 5.1.9 -   Leverage spot nodes with managed autoscaling </strong><a class="headerlink" href="#bp-519-leverage-spot-nodes-with-managed-autoscaling" title="Permanent link">&para;</a></h2>
<p>Enable managed autoscaling for your EMR clusters. From EMR 5.32 and EMR 6.2 there have been optimizations made to managed scaling to make it more resilient for your Spark workloads. It is not recommended to use Spot with core or master nodes since during reclaimation event, your cluster could be terminated and you would need to re-process all the work. Try to leverage task instance fleets with many instance types per fleet with Spot since it would give both cost and performance gains. However, in this case, make sure that your output is being written directly to EMRFS since we will have fixed core node on-demand capacity.</p>
<p>Following policy defines max core nodes to 2 and we are requesting the core nodes to be on-demand as recommended. Rest of the nodes are Spot task nodes.</p>
<p><img alt="BP - 9" src="../images/spark-bp-9.png" /></p>
<p>Following experimentation illustrates the performance gains using Managed Autoscaling.</p>
<p><img alt="BP - 10" src="../images/spark-bp-10.png" /></p>
<p>For Spark workloads, we observed ~50% gains compared to custom autoscaling clusters.</p>
<p><img alt="BP - 11" src="../images/spark-bp-11.png" /></p>
<p>Please note that the results may vary based on your workloads. Also, if your workloads are SLA sensitive and fault intolerant, it is best to use on-demand nodes for task fleets as well since reclaimation of Spot may lead to re-computation of one or many stages or tasks.</p>
<h2 id="bp-5110-for-workloads-with-predictable-pattern-consider-disabling-dynamic-allocation"><strong> BP 5.1.10  -   For workloads with predictable pattern, consider disabling dynamic allocation </strong><a class="headerlink" href="#bp-5110-for-workloads-with-predictable-pattern-consider-disabling-dynamic-allocation" title="Permanent link">&para;</a></h2>
<p>Dynamic allocation is enabled in EMR by default. It is a great feature for following types of workloads:</p>
<ol>
<li>Workloads processing variable amount of data  </li>
<li>When your cluster uses autoscaling</li>
<li>Dynamic processing requirements or unpredictable workload patterns</li>
<li>Streaming and ad-hoc workloads</li>
<li>When your cluster runs multiple applications</li>
<li>Your cluster is long-running</li>
</ol>
<p>The above would cover at least 95% of the workloads run by our customers today. However, there are a very few cases where:</p>
<ol>
<li>Workloads have a very predicatable pattern</li>
<li>Amount of data processed is predictable and consistent throughout the application</li>
<li>Cluster runs Spark application in batch mode</li>
<li>Clusters are transient and are of fixed size (no autoscaling)</li>
<li>Application processing is relatively uniform. Workload is not spikey in nature.</li>
</ol>
<p>For example, you may have a use case where you are collecting weather information of certain geo regions twice a day. In this case, your data load will be predictable and you will run two batch jobs per day - one at BOD and one at EOD. Also, you may use two transient clusters to process these two jobs.  </p>
<p>For such use cases, you can consider disabling dynamic allocation along with setting the precise  number and size of executors and vCores like below.</p>
<div class="codehilite"><pre><span></span><code>[{
    &quot;classification&quot;: &quot;spark-defaults&quot;,
    &quot;properties&quot;: {
        &quot;spark.dynamicAllocation.enabled&quot;: &quot;false&quot;,
        &quot;spark.executor.instances&quot;: &quot;12&quot;,
        &quot;spark.executor.memory&quot;: &quot;8G&quot;,
        &quot;spark.executor.cores&quot;: &quot;4&quot;
    },
    &quot;configurations&quot;: []
}]
</code></pre></div>

<p>Please note that if you are running more than one application at a time, you may need to tweak the configurations to allocate resources to them. By disabling dynamic allocation, Spark driver or YARN Application Master does not have to calculate resource requirements at runtime or collect certain heuristics. This may save anywhere from 5-10% of job execution time. However, you will need to carefully plan Spark executor configurations in order to ensure that your entire cluster is being utilized. If you choose to do this, then it is better to disable autoscaling since your cluster only runs a fixed number of executors at any given time unless your cluster runs other applications as well.</p>
<p>However, only consider this option if your workloads meet all the above criteria since otherwise your jobs may fail due to lack of resources or you may end up wasting your cluster resources.</p>
<h2 id="bp-5111-leverage-hdfs-as-temporary-storage-for-io-intensive-workloads"><strong> BP 5.1.11  -   Leverage HDFS as temporary storage for I/O intensive workloads </strong><a class="headerlink" href="#bp-5111-leverage-hdfs-as-temporary-storage-for-io-intensive-workloads" title="Permanent link">&para;</a></h2>
<p>Many EMR users directly read and write data in S3. This is generally suited for most type of use cases. However, for I/O intensive workflows, this approach could be slower - especially for heavy writes.</p>
<p><img alt="BP - 12" src="../images/spark-bp-12.png" /></p>
<p>For very I/O intensive workloads or for workloads where the temporary transform data is much larger than the final output, you can leverage HDFS as temporary storage and then use S3DistCp to copy the data into final location. For example, for a fraud detection use case where you could be performing transforms on TBs of data but your final output report is only a few KBs, in such scenarios, leveraging HDFS will give you better performance and also helps you avoid S3 throttling.</p>
<p><img alt="BP - 13" src="../images/spark-bp-13.png" /></p>
<p>Following is a typical application of HDFS for transient storage. A Spark context could be shared between multiple workflows, wherein, each workflow comprising of multiple transformations. After all transformations are complete, each workflow would write the output to HDFS location. Once all transforms are complete, you can save the final output to S3 either using S3DistCp or SDK determined by the number of files and output size.</p>
<p><img alt="BP - 14" src="../images/spark-bp-14.png" /></p>
<p>However,  while using this architecture, please make sure that you are sizing your HDFS properly to prevent job failures due to lack of storage space when the job is running. Refer to best practice BP 2.13 in Reliability section.</p>
<p>Even if you are using S3 directly to store your data, if your workloads are shuffle intensive, use storage optimized instances or SSD/NVMe based storage (for example: r5d’s and r6gd’s instead of r5s and r6g’s). This is because dynamic allocation will use Spark external shuffle service that spills data to local disks when the executor JVM cannot hold any more shuffle data. This process is a very I/O intensive one and will benefit from instance types that offer high disk throughput.</p>
<h2 id="bp-5112-spark-speculation-with-emrfs"><strong> BP 5.1.12  -   Spark speculation with EMRFS </strong><a class="headerlink" href="#bp-5112-spark-speculation-with-emrfs" title="Permanent link">&para;</a></h2>
<p>In Hadoop/Spark, speculative execution is a concept where a slower task will be launched in parallel on another node using a different JVM (based on resource availability). Whichever task completes first (original or speculated task), will write the output to S3. This works well for HDFS based writes. However, for EMRFS, turning on spark.speculation may lead to data loss or duplicate data. By default, “spark.speculation” is turned off. Only enable spark.speculation if you are doing one of the following.</p>
<ul>
<li>Writing Parquet files to S3 using EMRFSOutputCommitter</li>
<li>Using HDFS as temporary storage (in an understanding that final output will be written to S3 using S3DistCp)</li>
<li>Using HDFS as storage</li>
</ul>
<p><strong>Do not enable spark.speculation if none of the above criteria is met since it will lead to incorrect or missing or duplicate data in your destination.</strong></p>
<p>You can consider enabling spark.speculation especially while running workloads on very large clusters, provided you are performing one of the above actions. Reason is that, due to some hardware issues, one node out of 500+ nodes could be slower and may run tasks slowly even if data size being processed is the same as other tasks. Chances of this happening are higher in larger clusters. In that case, spark.speculation will help relaunch those slow tasks on other nodes providing SLA consistency (as long as the above criteria are met).</p>
<p>You can set spark.speculation to true in spark-defaults or pass it as a command line option (--conf spark.speculation="true").</p>
<div class="codehilite"><pre><span></span><code>[{
    &quot;classification&quot;: &quot;spark-defaults&quot;,
    &quot;properties&quot;: {
        &quot;spark.speculation&quot;: &quot;true&quot;
    },
    &quot;configurations&quot;: []
}]
</code></pre></div>

<p>Please do not enable spark.speculation if you are writing any non-Parquet files to S3 or if you are writing Parquet files to S3 without the default EMRFSOutputCommitter.</p>
<h2 id="bp-5113-data-quality-and-integrity-checks-with-deequ"><strong> BP 5.1.13 -   Data quality and integrity checks with deequ </strong><a class="headerlink" href="#bp-5113-data-quality-and-integrity-checks-with-deequ" title="Permanent link">&para;</a></h2>
<p>Spark and Hadoop frameworks do not inherently guarantee data integrity. While it is very rare, you may observe some data corruption or missing data or duplicate data due to unexpected errors in the hardware and software stack. It is highly recommended that you validate the integrity and quality of your data atleast once after your job execution. It would be best to check data correctness in multiple stages of your job - especially if your job is long-running.</p>
<p>In order to check your data integrity, consider using <a href="https://github.com/awslabs/deequ">Deequ</a> for your Spark workloads. Following are the blogs that can help you get started with Deequ for Spark workloads.</p>
<p><a href="https://aws.amazon.com/blogs/big-data/test-data-quality-at-scale-with-deequ/">Test data quality at scale with Deequ | AWS Big Data Blog</a>
<a href="https://aws.amazon.com/blogs/big-data/testing-data-quality-at-scale-with-pydeequ/">Testing data quality at scale with PyDeequ | AWS Big Data Blog</a></p>
<p>Sometimes, you may have to write your own validation logic. For example, if you are doing a lot of calculations or aggregations, you will need to compute twice and compare the two results for accuracy. In other cases, you may also implement checksum on data computed and compare it with the checksum on data written to disk or S3. If you see unexpected results, then check your Spark UI and see if you are getting too many errors from a single node by sorting the Task list based on "Status" and checking for error message of failed tasks. If you are seeing too many random unexpected errors such as "ArrayIndexOutOfBounds" or checksum errors from a single node, then it may be possible that the node is impaired. Exclude or terminate this node and re-start your job.</p>
<h2 id="bp-5114-use-dataframes-wherever-possible"><strong> BP 5.1.14 -   Use DataFrames wherever possible </strong><a class="headerlink" href="#bp-5114-use-dataframes-wherever-possible" title="Permanent link">&para;</a></h2>
<p>WKT we must use Dataframes and Datasets instead of RDDs since both have several enhancements over RDDs like catalyst optimizer and adaptive query execution. But between Datasets and Dataframes Dataframes perform certain optimizations during DAG creation and execution. These optimizations can be identified by inspecting the query plan. For example -</p>
<ul>
<li>Datasets perform many serializations and deserializations that Dataframes do not perform.</li>
<li>Dataframes perform better push downs. For example, if there is a filter operation, that is applied early on in the query plan so that the data transfer in-memory is reduced.</li>
<li>Dataframes avoid unnecessary exchanges. For example, distinct after join will be accomplished with two exchanges in datasets but with only one exchange in DFs.</li>
</ul>
<p>Only downside to using dataframes instead of datasets is that, with dataset, you generally define schema in class.</p>
<div class="codehilite"><pre><span></span><code>case class DeviceIoTData (
  battery_level: Long,
  c02_level: Long,
  cca2: String,
  cca3: String,
  cn: String,
  device_id: Long,
  device_name: String,
  humidity: Long,
  ip: String,
  latitude: Double,
  longitude: Double,
  scale: String,
  temp: Long,
  timestamp: Long
)
</code></pre></div>

<p>This provides you type-safety. When there are changes to your schema, it can be consolidated and tracked in a single class. This can be considered as an industry standard. While using Spark DDataframes, you can achieve something similar by maintaining the table columns in a list and fetching from that list dynamically in your code. But this requires some additional coding effort.</p>
<h2 id="bp-5115-data-skew"><strong> BP 5.1.15  -   Data Skew </strong><a class="headerlink" href="#bp-5115-data-skew" title="Permanent link">&para;</a></h2>
<p>Data skew can significantly slow down the processing since a single JVM could be handling a large amount of data. In this case observed in Spark UI, a single task is processing 25 times more data than other tasks. This can inevitably lead to slowness, OOMs and disk space filling issues.</p>
<p><img alt="BP - 15" src="../images/spark-bp-15.png" /></p>
<p>When there is a data skew, it is best handled at code level since very little can be done in terms of configuration. You can increase JVM size or use one fat executor per node in order to prevent OOMs to the best of ability. But this will impact other tasks and also will not improve your job performance since one task uses only one vCPU.</p>
<p>Following are some of the common strategies to mitigate data skew at code level.</p>
<h3 id="salting">Salting<a class="headerlink" href="#salting" title="Permanent link">&para;</a></h3>
<p>Salting is one of the most common skew mitigation technique where you add a "salt" to the column say "col1" that is skewed. You can split it into multiple columns like "col1_0","col1_1","col1_2" and so on. As number of salts increase, the skew decreases i.e., more parallelism of tasks can be achieved.</p>
<p><strong>Original data</strong></p>
<p><img alt="BP - 17" src="../images/spark-bp-17.png" /></p>
<p><strong>Salted 4  times</strong></p>
<p><img alt="BP - 18" src="../images/spark-bp-18.png" /></p>
<p><strong>Salted 8 times</strong></p>
<p><img alt="BP - 19" src="../images/spark-bp-19.png" /></p>
<p>A typical Salting workflow looks like below:</p>
<p><img alt="BP - 20" src="../images/spark-bp-20.png" /></p>
<p>For example, a salt column is added to the data with 100 randomized salts during narrow transformation phase (map or flatMap type of transforms).</p>
<div class="codehilite"><pre><span></span><code>n = 100
salted_df = df.withColumn(&quot;salt&quot;, (rand * n).cast(IntegerType))
</code></pre></div>

<p>Now, aggregation is performed on this salt column and the results are reduced by keys</p>
<div class="codehilite"><pre><span></span><code><span class="n">unsalted_df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">salted_df</span><span class="p">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s">&quot;salt&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">groupByFields</span><span class="p">).</span><span class="n">agg</span><span class="p">(</span><span class="n">aggregateFields</span><span class="p">).</span><span class="n">groupBy</span><span class="p">(</span><span class="n">groupByFields</span><span class="p">).</span><span class="n">agg</span><span class="p">(</span><span class="n">aggregateFields</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

<p>Similar logic can  be applied for windowing functions as well.</p>
<p>A downside to this approach is that it creates too many small tasks for non-skewed keys as well which may have a negative impact on the performance.</p>
<h3 id="isolated-salting">Isolated Salting<a class="headerlink" href="#isolated-salting" title="Permanent link">&para;</a></h3>
<p>This is an approach where salting is applied to only subset of the keys. If 80% or more data belongs to one field, isolated salting approach could be considered (for eg: skew due to NULL columns). In narrow transformation phase, we will isolate the skewed column. In the wide transformation phase, we  will isolate and reduce the heavily skewed column after salting. Finally, we will reduce other values without the salt and merge the results.</p>
<p>Isolated Salting workflow looks like below:</p>
<p><img alt="BP - 21" src="../images/spark-bp-21.png" /></p>
<p>An example code looks like below:</p>
<div class="codehilite"><pre><span></span><code><span class="nv">val</span> <span class="nv">count</span> <span class="o">=</span> <span class="mi">4</span>
<span class="nv">val</span> <span class="nv">salted</span> <span class="o">=</span> <span class="nv">df</span>.<span class="nv">withColumn</span><span class="ss">(</span><span class="s2">&quot;</span><span class="s">salt</span><span class="s2">&quot;</span>, <span class="nv">when</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">col === &quot;A&quot;, rand(1) * count cast IntegerType) otherwise 0)</span>
<span class="nv">val</span> <span class="nv">replicaDF</span> <span class="o">=</span> <span class="nv">skewDF</span>
      .<span class="nv">withColumn</span><span class="ss">(</span><span class="s2">&quot;</span><span class="s">replica</span><span class="s2">&quot;</span>, <span class="nv">when</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">col === &quot;A&quot;, (0 until count) toArray) otherwise Array(0))</span>
      .<span class="nv">withColumn</span><span class="ss">(</span><span class="s2">&quot;</span><span class="s">salt</span><span class="s2">&quot;</span>, <span class="nv">explode</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">replica</span><span class="s1">&#39;</span><span class="ss">))</span>
      .<span class="nv">drop</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">replica</span><span class="s1">&#39;</span><span class="ss">)</span>
<span class="nv">val</span> <span class="nv">merged</span> <span class="o">=</span> <span class="nv">salted</span>.<span class="nv">join</span><span class="ss">(</span><span class="nv">replicaDF</span>, <span class="nv">joinColumns</span> :<span class="o">+</span> <span class="s2">&quot;</span><span class="s">salt</span><span class="s2">&quot;</span><span class="ss">)</span>
</code></pre></div>

<h3 id="isolated-broadcast-join">Isolated broadcast join<a class="headerlink" href="#isolated-broadcast-join" title="Permanent link">&para;</a></h3>
<p>In this approach, smaller lookup table is broadcasted across workers and joined in map phase itself. Thus, reducing the amount of data shuffles. Similar to last approach, skewed keys are separated from normal keys. Then, we reduce the ”normal” keys and perform map-side join on isolated ”skewed” keys. Finally, we can merge the results of skewed and normal joins</p>
<p>Isolated map-side join workflow looks like below:</p>
<p><img alt="BP - 22" src="../images/spark-bp-22.png" /></p>
<p>An example code looks like below:</p>
<div class="codehilite"><pre><span></span><code>val count = 8
val salted = skewDF.withColumn(&quot;salt&quot;, when(&#39;col === &quot;A&quot;, rand(1) * count cast IntegerType) otherwise 0).repartition(&#39;col&#39;, &#39;salt&#39;) // Re-partition to remove skew
val broadcastDF = salted.join(broadcast(sourceDF), &quot;symbol&quot;)
</code></pre></div>

<h3 id="hashing-for-sparksql-queries">Hashing for SparkSQL queries<a class="headerlink" href="#hashing-for-sparksql-queries" title="Permanent link">&para;</a></h3>
<p>While running SparkSQL queries, you may have seen that it runs out of memory sometimes due to skew. Especially, this could be seen for windowing queries with a skew.</p>
<p>Following could be an example query with a skew.</p>
<div class="codehilite"><pre><span></span><code>select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey) AS row_num FROM testdb.skewlineitem
</code></pre></div>

<p>Considering there is a skew in l_orderkey field, we can split the above query into 4 hashes.</p>
<div class="codehilite"><pre><span></span><code>select * from (select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey) AS row_num FROM testdb.skewlineitem where cast(l_orderkey as integer)%4 = 1
union
select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey ) AS row_num FROM testdb.skewlineitem where cast(l_orderkey as integer)%4 = 2
union
select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey ) AS row_num FROM testdb.skewlineitem where cast(l_orderkey as integer)%4 = 3
union
select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey ) AS row_num FROM testdb.skewlineitem where cast(l_orderkey as integer)%4 = 4 )
limit 10;
</code></pre></div>

<p>If the values are highly skewed, then salting approaches should be used instead since this approach will still send all the skewed keys to a single task. This approach should be used to prevent OOMs quickly rather than to increase performance. The read job is re-computed for the number of sub queries written.</p>
<h2 id="bp-5116-use-right-type-of-join"><strong> BP 5.1.16  -   Use right type of join </strong><a class="headerlink" href="#bp-5116-use-right-type-of-join" title="Permanent link">&para;</a></h2>
<p>There are several types of joins in Spark. Some are more optimal than the other based on certain considerations.</p>
<h3 id="broadcast-join">Broadcast Join<a class="headerlink" href="#broadcast-join" title="Permanent link">&para;</a></h3>
<p>Broadcast joins are the most optimal options</p>
<h3 id="shuffle-hash-join">Shuffle Hash Join<a class="headerlink" href="#shuffle-hash-join" title="Permanent link">&para;</a></h3>
<h3 id="sort-merge-join">Sort Merge Join<a class="headerlink" href="#sort-merge-join" title="Permanent link">&para;</a></h3>
<h3 id="broadcast-nested-loop-join">Broadcast Nested Loop Join<a class="headerlink" href="#broadcast-nested-loop-join" title="Permanent link">&para;</a></h3>
<h2 id="bp-5117-configuring-spark-executor-blacklist"><strong> BP 5.1.17  - Configuring Spark Executor Blacklist </strong><a class="headerlink" href="#bp-5117-configuring-spark-executor-blacklist" title="Permanent link">&para;</a></h2>
<h2 id="bp-5118-configure-observability"><strong> BP 5.1.18 -   Configure observability </strong><a class="headerlink" href="#bp-5118-configure-observability" title="Permanent link">&para;</a></h2>
<p>Choose an observability platform based on your requirements.</p>
<h2 id="bp-5119-debugging-and-monitoring-spark-applications"><strong> BP 5.1.19  - Debugging and monitoring Spark applications </strong><a class="headerlink" href="#bp-5119-debugging-and-monitoring-spark-applications" title="Permanent link">&para;</a></h2>
<h2 id="bp-5120-common-errors"><strong> BP 5.1.20  -   Common Errors </strong><a class="headerlink" href="#bp-5120-common-errors" title="Permanent link">&para;</a></h2>
<ol>
<li><strong>Avoid 503 slow downs</strong></li>
<li>For mitigating S3 throttling errors, consider increasing fs.s3.maxRetries in emrfs-site configuration. By default, it is set to 15 and you may need to increase it based on your workload needs.</li>
<li>You can also increase the multipart upload threshold in EMRFS. Default value at which MPU triggers is 128 MB.</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="p">[{</span><span class="w"></span>
<span class="w">    </span><span class="s2">&quot;classification&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;emrfs-site&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="s2">&quot;properties&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="s2">&quot;fs.s3.maxRetries&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;20&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">        </span><span class="s2">&quot;fs.s3n.multipart.uploads.split.size&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;268435456&quot;</span><span class="w"></span>
<span class="w">    </span><span class="p">},</span><span class="w"></span>
<span class="w">    </span><span class="s2">&quot;configurations&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[]</span><span class="w"></span>
<span class="p">}]</span><span class="w"></span>
</code></pre></div>

<ul>
<li>Consider using Iceberg tables’ ObjectStoreLocationProvider to store data under [0*7FFFFF] prefixes and thus, help Amazon S3 learn write pattern to scale accordingly.</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="w"> </span><span class="nt">CREATE</span><span class="w"> </span><span class="nt">TABLE</span><span class="w"> </span><span class="nt">my_catalog</span><span class="p">.</span><span class="nc">my_ns</span><span class="p">.</span><span class="nc">my_table</span><span class="w"></span>
<span class="w"> </span><span class="o">(</span><span class="w"> </span><span class="nt">id</span><span class="w"> </span><span class="nt">bigint</span><span class="o">,</span><span class="w"></span>
<span class="w">   </span><span class="nt">data</span><span class="w"> </span><span class="nt">string</span><span class="o">,</span><span class="w"></span>
<span class="w">   </span><span class="nt">category</span><span class="w"> </span><span class="nt">string</span><span class="o">)</span><span class="w"></span>
<span class="w">   </span><span class="nt">USING</span><span class="w"> </span><span class="nt">iceberg</span><span class="w"> </span><span class="nt">OPTIONS</span><span class="w"></span>
<span class="w">   </span><span class="o">(</span><span class="w"> </span><span class="s1">&#39;write.object-storage.enabled&#39;</span><span class="o">=</span><span class="nt">true</span><span class="o">,</span><span class="w"></span>
<span class="w">     </span><span class="s1">&#39;write.data.path&#39;</span><span class="o">=</span><span class="s1">&#39;s3://my-table-data-bucket&#39;</span><span class="o">)</span><span class="w"></span>
<span class="w">     </span><span class="nt">PARTITIONED</span><span class="w"> </span><span class="nt">BY</span><span class="w"> </span><span class="o">(</span><span class="nt">category</span><span class="o">);</span><span class="w"></span>
</code></pre></div>

<p>Your S3 files will be arranged under MURMUR3 hash prefixes like below.</p>
<div class="codehilite"><pre><span></span><code> 2021-11-01 05:39:24  809.4 KiB 7ffbc860/my_ns/my_table/00328-1642-5ce681a7-dfe3-4751-ab10-37d7e58de08a-00015.parquet
 2021-11-01 06:00:10    6.1 MiB 7ffc1730/my_ns/my_table/00460-2631-983d19bf-6c1b-452c-8195-47e450dfad9d-00001.parquet
 2021-11-01 04:33:24    6.1 MiB 7ffeeb4e/my_ns/my_table/00156-781-9dbe3f08-0a1d-4733-bd90-9839a7ceda00-00002.parquet
</code></pre></div>

<ul>
<li>
<p>If using Iceberg is not an option and if above approaches don’t resolve the issue, you can create an AWS support case to partition your S3 prefixes. But the prefix pattern needs to be known in advance for eg: s3://bucket/000-fff/ or s3://bucket/<date fields from 2020-01-20 to 2030-01-20>/</p>
</li>
<li>
<p><strong>Increase event queue size and heartbeat interval for large number of executors</strong></p>
<ul>
<li>If the size of “dropped events</li>
</ul>
</li>
<li>
<p><strong>Increase HADOOP, YARN and HDFS heap sizes for intensive workflows</strong></p>
</li>
<li>
<p><strong>Use spark.yarn.archive to avoid compressing dependencies especially for high concurrency workloads</strong></p>
</li>
</ul>

              
            </article>
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="../introduction/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Introduction" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Introduction
            </div>
          </div>
        </a>
      
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../../..", "features": ["tabs"], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../../../assets/javascripts/workers/search.092fa1f6.min.js"}</script>
    
    
      <script src="../../../assets/javascripts/bundle.5a9542cf.min.js"></script>
      
    
  </body>
</html>